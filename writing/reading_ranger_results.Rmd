---
title: "Black Sea Bass: A Random Forest, reading in results"
author: "Min-Yang Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    fig_caption: yes
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    number_sections: yes
header-includes: \usepackage{setspace}\doublespacing
urlcolor: blue
editor_options:
  chunk_output_type: console
fontsize: 12pt
---

# Summary and Housekeeping


<!---- 
 The global_options chunk loads libraries, sets options, figures out if you're on a desktop or server, sets years, and sets graphing options
 **********************************************************************
* Purpose: 	Read in outputs of machine learning models 
* Inputs: These three can be adjusted by changing the "modeltype"
*   - Data file
*   - tuning results file
*   - final results file


* Outputs:
*   - Graphs of tuning quality and final model fit
*   - Aggregate predictions for the testing (validation) dataset and the entire dataset (train+test)


**********************************************************************

 
 --->
```{r global_options, include=FALSE}

modeltype<-"North_NOC_TEST"
# OR "nocluster", or "fiveclass", or "noc5class" OR "standard" or
# "South_NOC" or "North_NOC" "South_NOC_TEST" or "North_NOC_TEST"


library("here")

# load tidyverse and related
library("tidyverse")
library("scales")

# load tidyverse and related
library("tidymodels")


# load machine learning and estimation tools
library("nnet")
library("ranger")
library("partykit")
library("bonsai")
library("vip")

# load utilities
library("knitr")
library("kableExtra")
library("viridis")
library("conflicted")

#deal with conflicts
conflicts_prefer(dplyr::filter())
conflicts_prefer(dplyr::lag())
conflicts_prefer(purrr::discard())
conflicts_prefer(dplyr::group_rows())
conflicts_prefer(yardstick::spec())
conflicts_prefer(recipes::fixed())
conflicts_prefer(recipes::step())
conflicts_prefer(viridis::viridis_pal())


here::i_am("writing/reading_ranger_results.Rmd")


# You've estimated a few difference models, this sets the 
if (modeltype=="standard"){
  data_pattern<-"data_split"
  tuning_pattern<-"BSB_ranger_tune"
  final_pattern<-"BSB_ranger_results"
} else if (modeltype=="nocluster"){
  data_pattern<-"nocluster_data_split"
  tuning_pattern<-"BSB_ranger_nocluster_tune"
  final_pattern<-"BSB_ranger_nocluster_results"
}else if (modeltype=="fiveclass"){
  data_pattern<-"data_split_5_class"
  tuning_pattern<-"BSB_ranger_5class_tune"
  final_pattern<-"BSB_ranger_5class_results"
} else if (modeltype=="noc5class"){
  data_pattern<-"data_split_5_NOC_class"
  tuning_pattern<-"BSB_ranger_5_NOC_class_tune"
  final_pattern<-"BSB_ranger_5_NOC_class_results"
} else if (modeltype=="South_NOC"){
  data_pattern<-"nocluster_South_data_split"
  tuning_pattern<-"BSB_ranger_South_nocluster_tune"
  final_pattern<-"BSB_ranger_South_nocluster_results"
} else if (modeltype=="North_NOC"){
  data_pattern<-"nocluster_North_data_split"
  tuning_pattern<-"BSB_ranger_North_nocluster_tune"
  final_pattern<-"BSB_ranger_North_nocluster_results"
} else if (modeltype=="South_NOC_TEST"){
  data_pattern<-"nocluster_South_data_split_TEST"
  tuning_pattern<-"BSB_ranger_South_nocluster_tune_TEST"
  final_pattern<-"BSB_ranger_South_nocluster_results_TEST"
} else if (modeltype=="North_NOC_TEST"){
  data_pattern<-"nocluster_North_data_split_TEST"
  tuning_pattern<-"BSB_ranger_North_nocluster_tune_TEST"
  final_pattern<-"BSB_ranger_North_nocluster_results_TEST"
}else {
  stop("Unknown modeltype")
}





#############################################################################
#knitr options

knitr::opts_chunk$set(echo=TRUE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)

lbs_per_mt<-2204.62
#############################################################################
my_images<-here("images")
descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")


data_vintage_string<-list.files(here("results","ranger"), pattern=glob2rx(paste0(data_pattern,"*Rds")))
data_vintage_string<-gsub(data_pattern,"",data_vintage_string)
data_vintage_string<-gsub(".Rds","",data_vintage_string)
data_vintage_string<-max(data_vintage_string)


tuning_vintage<-list.files(here("results","ranger"), pattern=glob2rx(paste0(tuning_pattern,"*Rds")))
tuning_vintage<-gsub(tuning_pattern,"",tuning_vintage)
tuning_vintage<-gsub(".Rds","",tuning_vintage)
tuning_vintage<-max(tuning_vintage)


finalfit_vintage<-list.files(here("results","ranger"), pattern=glob2rx(paste0(final_pattern,"*Rds")))
finalfit_vintage<-gsub(final_pattern,"",finalfit_vintage)
finalfit_vintage<-gsub(".Rds","",finalfit_vintage)
finalfit_vintage<-max(finalfit_vintage)

vintage_string<-list.files(here("data_folder","main","commercial"), pattern=glob2rx("BSB_estimation_dataset*Rds"))

raw_oos_data_vintage_string<-list.files(here("data_folder","main","commercial"), pattern=glob2rx("BSB_unclassified_dataset*Rds"))
raw_oos_data_vintage_string<-gsub("BSB_unclassified_dataset","",raw_oos_data_vintage_string)
raw_oos_data_vintage_string<-gsub(".Rds","",raw_oos_data_vintage_string)
raw_oos_data_vintage_string<-max(raw_oos_data_vintage_string)



```

Data vintage is `r data_vintage_string`. Tuning vintage is `r tuning_vintage`.  Final fit vintage is `r finalfit_vintage`.

```{r load_in_data and tuning_results, include=FALSE}
# this was created with data_prep_ml.Rmd
data_split<-readr::read_rds(file=here("results","ranger",paste0(data_pattern,data_vintage_string,".Rds")))
tune_res<-readr::read_rds(file=here("results","ranger",paste0(tuning_pattern,tuning_vintage,".Rds")))
oos_data<-read_rds(file=here("data_folder","main","commercial",paste0("BSB_unclassified_dataset",raw_oos_data_vintage_string,".Rds")))

```

Brier classification. best  depends on the type of model.

```{r brier_class}

# eyeball the metrics 
all.metrics<-tune_res %>%
  collect_metrics() 

best_tree <- tune_res %>%
  select_best(metric = "brier_class")

opt_mtry<-best_tree$mtry

brier<-all.metrics %>%
  filter(.metric == "brier_class") %>%
  select(mean, mtry) %>%
  rename(brier_class=mean) 

 brier %>% ggplot(aes(mtry, brier_class)) +
  geom_point(show.legend = TRUE) 

 ggsave(here("results","ranger","tune",paste0("brier_class",modeltype,tuning_vintage,".png")), plot=last_plot())


 
  
brier<-tune_res %>%
  collect_metrics(summarize=FALSE) %>%
  filter(.metric == "brier_class") 

brier %>% ggplot(aes(x=as.factor(mtry), y=.estimate)) +
  geom_boxplot(show.legend = TRUE) 

  ggsave(here("results","ranger","tune",paste0("brier_class_indivd_",modeltype,tuning_vintage,".png")), plot=last_plot())

 
```

Multinomial log-loss best depends on the type of model.

```{r log_loss}
mn_log_loss<-all.metrics %>%
  filter(.metric == "mn_log_loss") %>%
#  filter(mtry <=10) %>%
  select(mean, mtry) %>%
  rename(mn_log_loss=mean)

 mn_log_loss %>% ggplot(aes(mtry, mn_log_loss)) +
  geom_point(show.legend = TRUE) 

 ggsave(here("results","ranger","tune",paste0("mn_log_loss",modeltype,tuning_vintage,".png")), plot=last_plot())

```


Are there any warnings from the tuning?

```{r warnings}
extracts<-tune_res %>%
  collect_notes()
extracts
not_good<-unique(extracts$id)
not_good
```
There are warnings in all the runs, all related to computation of precision.  All but 1 are due to "no predicted" smalls; one is due to no-predicted medium.


# Some in-sample results from Tuning

```{r roc}
# A ROC curve. I'm not sure if this is quite right, but I think it is.
predictions<-tune_res %>%
  collect_predictions()

# pull the probability names
prob_names<-colnames(predictions) 
prob_names<-grep("^\\.pred_", prob_names, value=TRUE)
prob_names<-grep("^\\.pred_class", prob_names, value=TRUE, invert=TRUE)


predictions %>%
  dplyr::filter(mtry==opt_mtry) %>%
  group_by(id, mtry) %>%
  #dplyr::filter(!id %in% not_good) %>%
  roc_curve(market_desc, any_of(prob_names)) %>%
  autoplot()



test2<-predictions %>%
  group_by(id, mtry) %>%
  dplyr::filter(mtry==opt_mtry) %>%
  #dplyr::filter(!id %in% not_good) %>%
  roc_curve(market_desc,any_of(prob_names))


ggsave(here("results","ranger","tune",paste0("roc_tune_",modeltype,tuning_vintage,".png")), plot=last_plot())


predictions %>%
  group_by(id, mtry) %>%
  dplyr::filter(mtry==opt_mtry)%>%
#  dplyr::filter(!id %in% not_good) %>%
  pr_curve(market_desc, any_of(prob_names)) %>%
  autoplot()

ggsave(here("results","ranger","tune",paste0("pr_curve_tune_",modeltype,tuning_vintage,".png")), plot=last_plot())


  
```

```{r load_final_model}

rm(tune_res)

final_fit<-read_rds(file=here("results","ranger",paste0(final_pattern,tuning_vintage,".Rds")))


# What is the workflow
final_workflow<-extract_workflow(final_fit)

# And the recipe
extract_recipe(final_fit)

preds<-final_fit$.workflow[[1]]$pre$actions$recipe$recipe$var_info %>%
 dplyr::filter(role=="predictor")

num_preds<-nrow(preds)

```


```{r final_predictions}
# Get predictions
final_predictions<-final_fit %>%
  collect_predictions()

# pull the probability names
prob_names<-colnames(predictions) 
prob_names<-grep("^\\.pred_", prob_names, value=TRUE)
prob_names<-grep("^\\.pred_class", prob_names, value=TRUE, invert=TRUE)


# look at the fit metrics
final_fit_metrics<-final_fit %>%
  collect_metrics()
final_fit_metrics

# Look at variable importance. I want to see all the predictors
final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = num_preds)
ggsave(here("results","ranger","final",paste0("vip",modeltype,finalfit_vintage,".png")), plot=last_plot())

# Reciever operator curve (True positive vs False Positive) for the final model evaluated on the validation data
final_predictions %>%
  roc_curve(market_desc, any_of(prob_names))  %>%
  autoplot()
ggsave(here("results","ranger","final",paste0("roc_",modeltype,finalfit_vintage,".png")), plot=last_plot())

# PR curve
final_predictions %>%
  pr_curve(market_desc,  any_of(prob_names)) %>%
  autoplot()
ggsave(here("results","ranger","final",paste0("pr_curve",modeltype,finalfit_vintage,".png")), plot=last_plot())
```


```{r predicted_testing_pounds}
# Compute predicted pounds for the testing dataset
#pull the test data, merge it with the predictions
test_data <- testing(data_split)
test_data<-test_data %>%
  rename(market_descOG=market_desc)
test_data<-cbind(test_data, final_predictions)

# keep just a few columns
test_data<-test_data %>%
  select(c(camsid, weighting, market_desc, lndlb, stockarea, year, market_descOG, any_of(prob_names), .pred_class))

#predicted pounds per transaction in the validation dataset.

for (l in c("Jumbo", "Large", "Medium", "Small", "Unclassified")) {
  tryCatch({
    test_data[[paste0("pred_", l)]] <- test_data[[paste0(".pred_", l)]] * test_data$lndlb
  }, error = function(e) {
    # R will just silently continue to the next iteration
  })
}


# This is basically a confusion matrix at the stockarea-year level.
 test_predictions<-test_data %>%
   group_by(stockarea, year,market_desc) %>%
  summarise(across(c(starts_with("pred"),"lndlb"), \(x) sum(x, na.rm = TRUE))) %>%
   ungroup()
test_predictions$modeltype<-modeltype
 
write_rds(test_predictions, file=here("results","ranger",paste0("aggregate_test_predictions_",modeltype,finalfit_vintage,".Rds")))
```

```{r table_of_Test_predictions}

# Aggregate to the stockarea- market category

 aggregate_test_predictions<-test_predictions %>%
  group_by(stockarea,market_desc) %>%
    summarise(across(c(pred_Jumbo,pred_Large,pred_Medium,pred_Small), sum))

# true landings by stockarea and market_category
trueTest<-test_predictions %>%
  mutate(market_desc=as.character(market_desc))%>%
    group_by(stockarea,market_desc) %>%
    summarise(lndlb=sum(lndlb))


# aggregate predictions, in pounds, by stockarea and market_category
Tp1<-test_predictions %>%
  group_by(stockarea) %>%
    summarise(across(c(pred_Jumbo,pred_Large,pred_Medium,pred_Small), sum))

Tp2<-Tp1 %>%
  pivot_longer(cols=!stockarea, names_to="market_desc", names_prefix="pred_",values_to="predicted")


Testmkt_preds<-Tp2 %>%
  left_join(trueTest, by=join_by(stockarea==stockarea, market_desc==market_desc))

Testmkt_preds<-Testmkt_preds %>%
  mutate(predicted=predicted/lbs_per_mt,
         lndlb=lndlb/lbs_per_mt) %>%
  rename(true_mt=lndlb,
         predicted_mt=predicted)%>%
  mutate(error=floor(100*(predicted_mt-true_mt)/true_mt))

Testmkt_preds$error=paste0(Testmkt_preds$error,"%")

 knitr::kable(Testmkt_preds, caption='Aggregate Predictions on the Test Dataset (mt)',format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))


```

```{r all_predictions}
full_data<-data_split$data
full_predictions<-predict(final_workflow, full_data, type="prob")

class_predictions<-predict(final_workflow, full_data)

full_predictions<-cbind(full_predictions, class_predictions)

full_predictions<-cbind(full_predictions, full_data)
write_rds(full_predictions, file=here("results","ranger",paste0("full_predictions_",modeltype,finalfit_vintage,".Rds")))

#predicted pounds per transaction in the validation dataset.

for (l in c("Jumbo", "Large", "Medium", "Small")) {
  tryCatch({
    full_predictions[[paste0("pred_", l)]] <- full_predictions[[paste0(".pred_", l)]] * full_predictions$lndlb
  }, error = function(e) {
    # R will just silently continue to the next iteration
  })
}

# Aggregate to the stockarea-year-market category
 aggregate_full_predictions<-full_predictions %>%
   group_by(stockarea, year,market_desc) %>%
  summarise(across(c(starts_with("pred"),"lndlb"), \(x) sum(x, na.rm = TRUE))) %>%
   ungroup()
aggregate_full_predictions$modeltype<-modeltype
 
write_rds(aggregate_full_predictions, file=here("results","ranger",paste0("aggregate_full_predictions_",modeltype,finalfit_vintage,".Rds")))

```


```{r table_of_Full_predictions}

# Aggregate to the stockarea- market category

 full_predictions<-aggregate_full_predictions %>%
  group_by(stockarea,market_desc) %>%
    summarise(across(c(pred_Jumbo,pred_Large,pred_Medium,pred_Small), sum))

# true landings by stockarea and market_category
true<-aggregate_full_predictions %>%
  mutate(market_desc=as.character(market_desc))%>%
    group_by(stockarea,market_desc) %>%
    summarise(lndlb=sum(lndlb))


# aggregate predictions, in pounds, by stockarea and market_category
p1<-full_predictions %>%
  group_by(stockarea) %>%
    summarise(across(c(pred_Jumbo,pred_Large,pred_Medium,pred_Small), sum))

p2<-p1 %>%
  pivot_longer(cols=!stockarea, names_to="market_desc", names_prefix="pred_",values_to="predicted")


mkt_preds<-p2 %>%
  left_join(true, by=join_by(stockarea==stockarea, market_desc==market_desc))

mkt_preds<-mkt_preds %>%
  mutate(predicted=predicted/lbs_per_mt,
         lndlb=lndlb/lbs_per_mt) %>%
  rename(true_mt=lndlb,
         predicted_mt=predicted)%>%
  mutate(error=floor(100*(predicted_mt-true_mt)/true_mt))

mkt_preds$error=paste0(mkt_preds$error,"%")

 knitr::kable(mkt_preds, caption='Predictions on the Full Dataset (mt)',format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))





```



<!---
\newpage
--->
# References
<div id="refs"></div>





# Appendix{-}