---
title: "Black Sea Bass: A Random Forest, reading in results"
author: "Min-Yang Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    fig_caption: yes
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    number_sections: yes
header-includes: \usepackage{setspace}\doublespacing
urlcolor: blue
editor_options:
  chunk_output_type: console
fontsize: 12pt
---

# Summary and Housekeeping


<!---- 
 The global_options chunk loads libraries, sets options, figures out if you're on a desktop or server, sets years, and sets graphing options
 --->
```{r global_options, include=FALSE}


library("here")

# load tidyverse and related
library("tidyverse")
library("scales")

# load tidyverse and related
library("tidymodels")


# load machine learning and estimation tools
library("nnet")
library("ranger")
library("partykit")
library("bonsai")
library("vip")

# load utilities
library("knitr")
library("kableExtra")
library("viridis")
library("conflicted")

#deal with conflicts
conflicts_prefer(dplyr::filter())
conflicts_prefer(dplyr::lag())
conflicts_prefer(purrr::discard())
conflicts_prefer(dplyr::group_rows())
conflicts_prefer(yardstick::spec())
conflicts_prefer(recipes::fixed())
conflicts_prefer(recipes::step())
conflicts_prefer(viridis::viridis_pal())


here::i_am("writing/reading_ranger_results.Rmd")






#############################################################################
#knitr options

knitr::opts_chunk$set(echo=FALSE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)

lbs_per_mt<-2204.62
#############################################################################
my_images<-here("images")
descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")


data_vintage_string<-list.files(here("data_folder","main","commercial"), pattern=glob2rx("BSB_estimation_dataset*Rds"))
data_vintage_string<-gsub("BSB_estimation_dataset","",data_vintage_string)
data_vintage_string<-gsub(".Rds","",data_vintage_string)
data_vintage_string<-max(data_vintage_string)


tuning_vintage<-list.files(here("results","ranger"), pattern=glob2rx("BSB_ranger_tune*Rds"))
tuning_vintage<-gsub("BSB_ranger_tune","",tuning_vintage)
tuning_vintage<-gsub(".Rds","",tuning_vintage)
tuning_vintage<-max(tuning_vintage)


finalfit_vintage<-list.files(here("results","ranger"), pattern=glob2rx("BSB_ranger_results*Rds"))
finalfit_vintage<-gsub("BSB_ranger_results","",finalfit_vintage)
finalfit_vintage<-gsub(".Rds","",finalfit_vintage)
finalfit_vintage<-max(finalfit_vintage)


```

Data vintage is `r data_vintage_string`. Tuning vintage is `r tuning_vintage`.  Final fit vintage is `r finalfit_vintage`.

```{r load_in_data and results, include=FALSE}
# this was created with data_prep_ml.Rmd
estimation_dataset<-readr::read_rds(file=here("data_folder","main","commercial",paste0("BSB_estimation_dataset",data_vintage_string,".Rds")))
tune_res<-readr::read_rds(file=here("results","ranger",paste0("BSB_ranger_tune",tuning_vintage,".Rds")))

final_fit<-readr::read_rds(file=here("results","ranger",paste0("BSB_ranger_results",tuning_vintage,".Rds")))



```

Brier classification suggests an mtry around 13.

```{r hyper_parameter_tuning_results}

# eyeball the metrics 
all.metrics<-tune_res %>%
  collect_metrics() 

all.metrics %>%
  filter(.metric == "brier_class") %>%
#  filter(mtry <=15) %>%

  select(mean, mtry) %>%
  pivot_longer(mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "brier_class")
ggsave(here("results","ranger","tune",paste0("brier_class.png",tuning_vintage,".png")), plot=last_plot())

```

Multinomial log-loss suggests mtry of 9.  There might be another dip round 15-20, so might need to try again there.

```{r hyper_parameter_tuning_results}
all.metrics %>%
  filter(.metric == "mn_log_loss") %>%
#  filter(mtry <=10) %>%
  select(mean, mtry) %>%
  pivot_longer(mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "mn_log_loss")
ggsave(here("results","ranger","tune",paste0("mn_log_loss.png",tuning_vintage,".png")), plot=last_plot())

```


Are there any warnings from the tuning?

```{r warnings}
extracts<-tune_res %>%
  collect_notes()
extracts
not_good<-unique(extracts$id)
not_good
```
Yes, a few resamples have some odd results. In Resamples 2 and 4, Small is never predicted, but there are some small that actually occur.  When this happens, Precision is undefined.

In other resamples, there is difficulty computing Sensitivity or Recall because there are no "true" events (like no smalls).

# Some in-sample results from Tuning

```{r roc}
# A ROC curve. I'm not sure if this is quite right, but I think it is.
predictions<-tune_res %>%
  collect_predictions()


predictions %>%
  group_by(id, mtry) %>%
  dplyr::filter(mtry==9) %>%
  #dplyr::filter(!id %in% not_good) %>%
  roc_curve(market_desc, .pred_Jumbo:.pred_Small) %>%
  autoplot()

ggsave(here("results","ranger","tune",paste0("roc_tune_",tuning_vintage,".png")), plot=last_plot())


predictions %>%
  group_by(id, mtry) %>%
  dplyr::filter(mtry==9)%>%
#  dplyr::filter(!id %in% not_good) %>%
  pr_curve(market_desc, .pred_Jumbo:.pred_Small) %>%
  autoplot()

ggsave(here("results","ranger","tune",paste0("pr_curve_tune_",tuning_vintage,".png")), plot=last_plot())


  
```

```{r }
# Get predictions
final_predictions<-final_fit %>%
  collect_predictions()

# look at the fit metrics
final_fit_metrics<-final_fit %>%
  collect_metrics()

# Look at variable importance
final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
ggsave(here("results","ranger","final",paste0("vip",finalfit_vintage,".png")), plot=last_plot())


final_predictions %>%
  roc_curve(market_desc, .pred_Jumbo:.pred_Small) %>%
  autoplot()
ggsave(here("results","ranger","final",paste0("roc_",finalfit_vintage,".png")), plot=last_plot())


final_predictions %>%
  pr_curve(market_desc, .pred_Jumbo:.pred_Small) %>%
  autoplot()
ggsave(here("results","ranger","final",paste0("pr_curve",finalfit_vintage,".png")), plot=last_plot())


# I have "true", I just need to merge in "weighting"  

# 
# weighted_final_predictions<-final_predictions %>%
#   mutate(predJ=.pred_Jumbo*weighting,
#          predL=.pred_Large*weighting,
#          predM=.pred_Medium*weighting,
#          predS=.pred_Small*weighting)
# 
# 
# aggregate_predictions<-predictions %>%
#   group_by(market_desc, year) %>%
#   summarise(predJ=sum(predJ),
#          predL=sum(predJ),
#          predM=sum(predJ),
#          predS=sum(predJ),
#          true_pounds=sum(weighting)
  )
```




<!---
\newpage
--->
# References
<div id="refs"></div>





# Appendix{-}