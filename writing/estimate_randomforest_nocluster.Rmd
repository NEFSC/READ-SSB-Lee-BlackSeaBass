---
title: "Black Sea Bass: A Random Forest"
author: "Min-Yang Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    fig_caption: yes
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    number_sections: yes
header-includes: \usepackage{setspace}\doublespacing
urlcolor: blue
editor_options:
  chunk_output_type: console
fontsize: 12pt
---

# Summary, Housekeeping, and Overview

I'm using the tidymodels framework to train and test the classification trees and
random forest.  The main advantage is that switching models or estimation packages
(partykit::ctree vs ranger vs randomForest for example) is easier. Writing the model 
uses tidy syntax.  Tuning the model is made easier by using tune and yardstick.
Fitting ranger requires bonsai.

The canonical way to do this is to declare a recipe and a workflow.  Ideally,
everything would be part of the workflow, but my data processing skills in R are
note good enough to want to do this.  Therefore, I'm basically passing the
recipe into the workflow. C'est la guerre.

<!---- 
 The global_options chunk loads libraries, sets options, figures out if you're on a desktop or server, sets years, and sets graphing options
 
 If you want to run this from the command line
 
 Rscript -e 'library(rmarkdown); rmarkdown::render("./writing/estimate_randomforest.Rmd", "html_document")' 
 
 This doesn't compile to pdf on the containers (pdflatex not found)
 
 
 
 
 --->
```{r global_options, include=FALSE}
# Set these two to control the size of the dataset. Useful for making sure code 
# works.
testing<-FALSE
testing_fraction<-.25


library("here")

# load tidyverse and related
library("tidyverse")
library("scales")

# load tidyverse and related
library("tidymodels")

# load machine learning and estimation tools
# ranger imports RcppEigen and Rcpp, all 3 need to be compiled on unix.
# you might want to install Rcpp, then RcppEigen, then ranger

library("nnet")
library("ranger")

library("partykit")
library("bonsai")
# load utilities
library("knitr")
library("kableExtra")
library("viridis")
library("conflicted")

#deal with conflicts
conflicts_prefer(dplyr::filter())
conflicts_prefer(dplyr::lag())
conflicts_prefer(purrr::discard())
conflicts_prefer(dplyr::group_rows())
conflicts_prefer(yardstick::spec())
conflicts_prefer(recipes::fixed())
conflicts_prefer(recipes::step())
conflicts_prefer(viridis::viridis_pal())

here::i_am("writing/estimate_randomforest_nocluster.Rmd")


# Determine what platform the code is running on and set the number of threads for ranger
platform <- Sys.info()['sysname']
# check the name of the effective_user
if(platform == 'Linux'){
  if(Sys.info()['effective_user'] %in% c("mlee")){
      runClass<-'Container'
    }
    else{
      runClass <- 'Local'
    }
  }

if(platform == 'Windows'){
  runClass<-'Windows'
}

if (runClass %in% c('Local', 'Windows')){
  my.ranger.threads<-6
} else if (runClass %in% c('Container')){ 
  my.ranger.threads<-8

  }












#############################################################################
#knitr options

knitr::opts_chunk$set(echo=TRUE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)

lbs_per_mt<-2204.62
#############################################################################
my_images<-here("images")
descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")
vintage_string<-list.files(here("data_folder","main","commercial"), pattern=glob2rx("BSB_estimation_dataset*Rds"))
vintage_string<-gsub("BSB_estimation_dataset","",vintage_string)
vintage_string<-gsub(".Rds","",vintage_string)
vintage_string<-max(vintage_string)
estimation_vintage<-as.character(Sys.Date())
```


Most of my data cleaning code is in stata. There's no reason to port it to R and risk mistakes now.  In brief, I:

1. Extract transaction level commercial landings of black sea bass at the camisd+subtrip level (cams_land.rec=0). Any column in CAMS_LAND is available, but sales transactions are tied to a "trip", not a "subtrip". This means there is some uncomfortableness for any transactions corresponding to multi-area (and multi-gear) trips. 
2. I do some "joins" to keyfiles (market category, market grade, gear, and economic deflators).
3. I do some tidying-up (converting datetime variables to date variables)
4. I rebin status=DLR_ORPHAN_SPECIES into status=MATCH

5. There is a little data dropping
  1. landed pounds=0
  2. Some landings from VA and DE that look like aggregates. 
6. I do some binning of gears, loosely into
  1. Line or Hand gear
  2. Trawls
  3. Gillnets
  4. Pot and Trap
  5. Misc=Dredge, Seine, and Unknown.
  
7.  I do some binning of market categories
  1. Unclassified and "Mixed or Unsized" are combined
  2. Small, Extra Small, and Pee Wee (Rats) are combined
  3. Medium and "Medium or Select" are combined.
8.  Ungraded is combined with Round
9. I construct a stockunit indicator
  1. south is 621 and greater, plus 614 and 615 
  2. North is 613 and smaller, plus 616
10. I create a semester indicator (=1 if Jan to June and =2 if July to Dec)
11. I SHOULD scale landed pounds, nominal value, and deflated value to "thousands". Prices
are in both real and nominal dollars per landed pound. 
12. I have day-marketcategory landings (pounds) by "other vessels". I also have day-state-marketcategory and day-stockarea-marketcategory. 

```{r load_in_data}
# this was created with data_prep_ml.Rmd
estimation_dataset<-readr::read_rds(file=here("data_folder","main","commercial",paste0("BSB_estimation_dataset",vintage_string,".Rds")))

```



```{r ml_data_setup, include=TRUE}

 # for reproducibility
 set.seed(4587315)

 # remove obs with missing prices,
 estimation_dataset<-estimation_dataset %>%
   dplyr::filter(is.na(price)==0) %>%
   mutate(dlrid=forcats::as_factor(dlrid)) %>%
    mutate(market_desc=forcats::fct_relevel(market_desc,c("Jumbo","Large","Medium","Small"))
    )

# When testing, take a subset of the data. This is just to test how my code is working   
if(testing==TRUE){
 estimation_dataset$rand<-runif(nrow(estimation_dataset))
 estimation_dataset<-estimation_dataset %>%
     dplyr::filter(rand<=testing_fraction)
}

 # construct the "case weights" variable here.
 estimation_dataset<-estimation_dataset %>%
     mutate(weighting = frequency_weights(weighting))

set.seed(2824)
# 80% of the data in the training, and 20% in the holdout sample, not weighted.
# split on strata=market_desc, although I don't think this is strictly necessary. 
data_split <- initial_split(data=estimation_dataset, prop=0.8, strata=market_desc) 
train_data <- training(data_split)
test_data <- testing(data_split)

nrow(train_data)
nrow(test_data)
readr::write_rds(data_split, file=here("results","ranger",paste0("nocluster_data_split",estimation_vintage,".Rds")))



```

# Recipe definition

The recipe simply defines the dataset, outcome (reponse, y) variable, id variables,
and predictor variables.


```{r, recipe}

# Use a recipe on the training data. 

# assign roles to predictors, outcome, groups, and weights
BSB.Classification.Recipe <- recipe(train_data) %>%
  update_role(market_desc, new_role = "outcome")%>%
  update_role(c(dlrid,camsid), new_role = "ID variable") %>%
  update_role(c(mygear,price,stockarea, state, year, month, semester, lndlb, grade_desc, trip_level_BSB), new_role = "predictor")

# State-level daily Landings on "other" trips, by market category  
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(StateOtherQJumbo, StateOtherQLarge, StateOtherQMedium, StateOtherQSmall), new_role = "predictor") 

# stockarea-level daily Landings on "other" trips, by market category  
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(StockareaOtherQJumbo, StockareaOtherQLarge, StockareaOtherQMedium, StockareaOtherQSmall), new_role = "predictor") 

# Trailing 7 days landings, by stockarea and market category   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_StockareaQJumbo, MA7_StockareaQLarge, MA7_StockareaQMedium, MA7_StockareaQSmall), new_role = "predictor")

# Trailing 7 days landings, by state and market category   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_StateQJumbo, MA7_StateQLarge, MA7_StateQMedium, MA7_StateQSmall), new_role = "predictor") 

# Trailing 7 day trips, by state and stock area.   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_stockarea_trips, MA7_state_trips), new_role = "predictor") 

# Dealer share of landings by market category from 2013-2017   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(Share2014Jumbo, Share2014Large, Share2014Medium,Share2014Small, Share2014Unclassified), new_role = "predictor") 

# Dealer transaction count of landings by market category from 2013-2017   

BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(TransactionCountJumbo, TransactionCountLarge, TransactionCountMedium,TransactionCountSmall, TransactionCountUnclassified), new_role = "predictor") 





# You can't center the factor variables
# rescale and recenter 
BSB.Classification.Recipe <- BSB.Classification.Recipe %>% 
#  step_impute_knn(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

recipe_summary<-BSB.Classification.Recipe %>%
  summary() %>%
  arrange(source,role, variable)

recipe_summary

#How many predictors
npredict<-nrow(recipe_summary %>% dplyr::filter(role=="predictor"))

```


# Model Definition and Baseline Fit

The model definition step declares the type of model (classification), engine (ranger), 
and any options.  For this section, I'm setting 500 trees in the RF, no fewer than 5
observations at the end of a branch, and 3 randomly selected variables.

I have left most of these options at their defaults. I'm noting things here 

Missing values are handled with the "na.learn" option (default). 

>With na.action = "na.learn", missing values are ignored for calculating an 
initial split criterion value (i.e., decrease of impurity). Then for the best 
split, all missings are tried in both child nodes and the choice is made based 
again on the split criterion value.

@Hastie2009: The first is applicable to categorical predictors: we simply make a new category for “missing.”
Second is construction of surrogate variables. When considering a predictor for a split, we use only the observations for which that predictor is not missing. Having chosen the best (primary) predictor and split point, we form a list of surrogate predictors and split points. The first surrogate is the predictor and corresponding split point that best mimics the split of the training data achieved by the primary split.  This is basically na.learn.

probability=FALSE. A probability forest (Malley et al 2012) might be a good idea

importance=NULL and splitrule=NULL.  This uses the Gini index as the impurity measure.

Geurts et al(2006)'s extremely random trees can be set with splitrule="extratrees".

trees=500. I don't have a good rationale for choose this.

Need to investigate the handling of the unordered factor covariates (@Hastie2009; Coppersmith et al 1999)

For an unordered predictor with $q$ levels, there are $2^{q-1}-1$ different ways to partition into two groups. This is not great.  For One way to handle this is to order the predictor levels in decreasing order of frequency and treat as if the factor was ordered.  This is proven to be the optimal way to handle unordered predictors for binary and quantitative (numeric) dependent variables.  It is not for multicategory outcomes @Hastie2009.

The partitioning algorithm tends to favor categorical predictors with many levels q; the number of partitions grows exponentially in q, and the more choices we have, the more likely we can find a good one for the data at hand. This can lead to severe overfitting if q is large, and such variables should be avoided @Hastie2009.

  * Coppersmith D., Hong S. J., Hosking J. R. (1999). Partitioning nominal attributes in decision
trees. Data Min Knowl Discov 3:197-217. doi:10.1023/A:1009869804967.

```{r define a model}

ranger_model<-rand_forest(mode="classification", trees = 500, min_n=5, mtry=3) %>%
  set_engine("ranger",num.threads=!!my.ranger.threads, na.action="na.learn", respect.unordered.factors="order", importance="impurity")


case_weights_allowed(ranger_model)

#cf_model<-rand_forest(mode="classification", 
#                    trees = 500, min_n=5, mtry=3) %>%
#  set_engine("partykit")
#case_weights_allowed(cf_model)

```

The workflow is the combination of data processing and model statement

```{r define_workflow}
# Use a workflow that combines the data processing recipe, assigns weights, and the model configuation

BSB.Ranger.Workflow <-
  workflow() %>%
  add_model(ranger_model) %>% 
  add_recipe(BSB.Classification.Recipe)


# BSB.cf.Workflow <-
#   workflow() %>%
#   add_model(cf_model) %>% 
#   add_recipe(BSB.Classification.Recipe)

```




```{r fit_model, eval=FALSE}
set.seed(234)
# Fit the model on the entire training dataset. This of course, is not the best 
# thing to do.  There's 2 parameters that we should search over mtry and min_n
# to find the best model. But this is *a* model.

start_time_1fit<-Sys.time()

rf_fit <- 
  BSB.Ranger.Workflow %>% 
  fit(data = train_data)

end_time_1fit<-Sys.time()

end_time_1fit-start_time_1fit

```


## Tuning

We will do 10 fold cross validation over the mtry and trees set of parameters. We'll hold
trees constant. The number of trees isn't the most important thing, but we should eventually search over it.


```{r hyper_parameter_tuning}


set.seed(457)
# split the training data group wise into 10 folds with the same number of observations, but grouped by dlrid, so that each dlrid is wholly contained in a single fold.
myfolds<-rsample::vfold_cv(train_data, strata=market_desc, v = 10)

# Set up a set of mtry to search over. This is not a great grid, but it's fine for now
#rf_grid <- grid_regular(
#     mtry(range = c(1, 20)), levels=10)
#      trees(range=c(100,1000), levels=5)
#rf_grid<-rbind(rf_grid,60)
#rf_grid


mtry<-1:20
mtry<-c(mtry,25,npredict)
rf_grid<-as.data.frame(mtry)



# configure the tuning part of the model.
tune_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = 5,
) %>%
  set_mode("classification") %>%
  set_engine("ranger",num.threads=!!my.ranger.threads, na.action="na.learn", respect.unordered.factors="order", importance="impurity")





# make a turning workflow. This combines the BSB.Classification.Recipe "data declaration" steps and new "tuning"
# steps as the model.
tune_wf <- workflow() %>%
  add_recipe(BSB.Classification.Recipe) %>%
  add_model(tune_spec)

hardhat::extract_parameter_set_dials(tune_wf)

# pass in a bunch of metrics
# if the recipe/workflow is case_weight aware, the metrics are also case-weight aware
class_and_probs_metrics <- metric_set(sensitivity, specificity, precision, bal_accuracy, mn_log_loss,average_precision, accuracy, brier_class, roc_auc)

# Tune the model by passing in the workflow, folds and the rf_grid
# I can look at the canned metrics in  https://yardstick.tidymodels.org/articles/metric-types.html
# I'm more interested in the metrics based on "soft" predictions (class_prob). 
# I will use the class probabilities on the back end, not a 'hard' prediction 
# I'm interested in the 'aggregate predictions', not the 



rf_control_grid<-control_grid(save_pred = TRUE, parallel_over="everything")
start_time_tune<-Sys.time()

tune_res <- tune_grid(
  tune_wf,
  resamples = myfolds,
  grid = rf_grid,
  control=rf_control_grid,
  metrics=class_and_probs_metrics
)


#An alternative that searches over more things
#Search over mtry and trees
# rf_grid2 <- grid_regular(
#      mtry(range = c(1, 20)),
#      trees(range=c(500,1500)),
#      levels=c(20,3)
# )


# configure the tuning part of the model.
# tune_spec2 <- rand_forest(
#   mtry = tune(),
#   trees = tune(),
#   min_n = 5,
# ) %>%
#   set_mode("classification") %>%
#   set_engine("ranger",num.threads=!!my.ranger.threads, na.action="na.learn", respect.unordered.factors="order", importance="impurity")
# 
# 
# 
# 
# # make a turning workflow. This combines the BSB.Classification.Recipe "data declaration" steps and new "tuning"
# # steps as the model.
# tune_wf2 <- workflow() %>%
#   add_recipe(BSB.Classification.Recipe) %>%
#   add_model(tune_spec2)
# # search over mtry and trees
# 
# 
# hardhat::extract_parameter_set_dials(tune_wf2)
# 
# 
# start_time_tune<-Sys.time()
# 
# tune_res <- tune_grid(
#   tune_wf2,
#   resamples = myfolds,
#   grid = rf_grid2,
#   control=rf_control_grid,
#   metrics=class_and_probs_metrics
# )
# 
# 
# 
# end_time_tune<-Sys.time()

readr::write_rds(tune_res, file=here("results","ranger",paste0("BSB_ranger_nocluster_tune",estimation_vintage,".Rds")))

end_time_tune-start_time_tune

```

Select the best Rforest based on log loss from the 10 folds.  Do a final fit on the full training dataset, predict on the validation dataset. Save the data

```{r FinalModelFit}
# Add other metrics 

best_tree <- tune_res %>%
  select_best(metric = "mn_log_loss")

best_tree

# finalize model by picking the best model hyperparameters
final_wf <- 
  tune_wf %>% 
  finalize_workflow(best_tree)


# Final model fitting on the full training dataset 
final_fit <- 
  final_wf %>%
  last_fit(data_split, metrics=class_and_probs_metrics) 

readr::write_rds(final_fit, file=here("results","ranger",paste0("BSB_ranger_nocluster_results",estimation_vintage,".Rds")))
```

Print out the final fit metrics.

```{r fit_metrics}

# print out the metrics
final_fit %>%
  collect_metrics()


```

```{r}
Sys.time()


sessionInfo()
```

<!---
\newpage
--->
# References
<div id="refs"></div>





# Appendix{-}