---
title: 'Economic Informed Stock Assessments: Using Prices to Understand the Length of Fish'
author: "Min-Yang Lee and Emily Liljestrand"
date: "`r format(Sys.time(), '%B %d, %Y')`" 
documentclass: article
csl: "../ajae_mod.csl"
abstract: "The length- and age-structure of commercial landings is a crucial input to fish stock assessment. In the Northeast United States, NOAA Fisheries deploys samplers to fish dealers to measure the length of commercially landed fish.  Gaps in the data can arise when categories of certain fish simply are not sampled.  When this occurs, the information contained in fish prices can be used to fill in information about the size of these unmeasured fish. The size of individual fish is a characteristic than determines the price of fish; larger specimens nearly always receive higher prices than smaller ones. I use the Black Sea Bass fishery in the Northeast United States to illustrate my approach to using economic data to inform stock assessments.  There are 5 prevailing market categories: Jumbo, Large, Medium, Small, and Unclassified.  From 2020 to 2023, 5 to 10% of commercial landings were in the “Unclassified” market category; but no fish in this category were measured. I estimate a hedonic model to confirm that individual size impacts fish prices: “Unclassified” Black Sea Bass are priced between “Large” and “Medium” fish. Then I estimate a multinomial logit that explains market category as a function of prices and other attributes.  I use the results to predict the probability of the “Unclassified” belonging to any of the other market categories, allowing me to construct a price-informed length structure of commercial removals. "
keywords: "Hedonic Prices, Age structure, Stock assessment"
output:
  pdf_document: 
    includes:
      in_header: "../preamble-latex.tex"
    keep_tex: yes
    pandoc_args: --pdf-engine=pdflatex
    number_sections: true
  word_document: null
  html_document: null
fontsize: 12pt
bibliography: "C:/Users/min-yang.lee/Documents/library.bib"
---


 <!---- 
 The hardcoded bibilography is ghastly. 
Summary and Housekeeping

I did my data processing with  ``\stata_code\data_extraction_processing\extraction\commercial\00_cams_extraction.do`` and ``00_extraction_wrapper.do``

I also ran many of the  files in ``\stata_code\data_extraction_processing\analysis`` to do some exploratory analysis. 

Warnings: There is something funky going on with VA "status=PZERO" starting in 2021.  There is also something funky going on with Delaware landings, but there are no hullids/permit numbers so I don't think this will throw anything off at the vessel level.

--->

```{r global_options, include=FALSE}

 library("foreign")
 library("here")
 library("tidyverse")
 library("scales")
 library("knitr")
 library("lubridate")
 library("kableExtra")
 library("haven")
 library("mapview")
 library("sf")
library("htmltools")
here::i_am("writing/Economic_informed_stock_assessments.Rmd")

#############################################################################
#knitr options

knitr::opts_chunk$set(echo=FALSE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)


#############################################################################
my_images<-here("images")

descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")


# Read in statistical areas
stat_areas_location<-here("data_folder","external","shapefiles","Statistical_Areas_2010.shp")
stat_areas <- st_read(dsn = stat_areas_location)

#trim out some stat areas that I don't care about.
stat_areas<-stat_areas %>%
  dplyr::filter(Id>=460) %>% # Canada
   dplyr::filter(Id<=711) %>% # SERO
  dplyr::filter(!Id %in% c(640,650,660,670,680) ) #Offshore

```

# Introduction

<!--- Include the next 2 child documents.   --->
```{r, child=here("writing", c("summary.Rmd")),  eval=FALSE}
```




# Background

<!--- Include the next 2 child documents.   --->
```{r, child=here("writing", c("BSB_history.Rmd", "BSB_economic_background.Rmd")),  eval=FALSE}
```

## Black Sea Bass and the Stock Assessment Enterprise 

The age-structure of commercial landings is a crucial input to the stock assessment. Samplers measure the length of commercially landed fish and collect otoliths.  Analysts request coverage levels of different market categories. Gaps in the data can arise when categories of certain fish  are not sampled.  When this occurs, the information contained in fish prices can be used to fill in information about the size of these unmeasured fish. 

Need to justify having these as explanatory variables (predictors).
Stock area
Semester


```{r borrowed_lengths, fig.show = "hold", out.width = "48%", fig.cap="Borrowed Lengths in Black Sea Bass",  fig.align = "center", echo=FALSE}
knitr::include_graphics(here("images","background",c("2023_BSB_UNIT_TOR2_commercialdata_slide8.jpg")))
```

Slide 8 from ``2023_BSB_UNIT_ppt_TOR2_commercialdata.pdf`` illustrates the problem generally.  Pink in the bar graphs are good, this means there were enough measured fish in a market category.  Everything else indicates a certain amount of "borrowing" of data, where lengths are taken from market-area-semester combination.  

The underlying data is taken from **STOCKEFF**iciency, which originally used CFDBS and other sources. For more recent data, it points at "CAMS.CFDBS" style tables, which relies on CAMS.  It also does the area allocation also. It does some intermediate aggregation.  Then Emily pulls it from the stockeff webpage.  To bring anything to production probably will require pushing something to oracle.

For our purposes, we do not care about predicting a single "unclassified" observation well.  The research question is related to an aggregate of predictions: landings aggregated over a stockarea-year-semester (or perhaps stockarea-year).  

## Regulatory Environment

[only the details needed to establish why we have certain RHS variables in the the model]

State -- regulations vary by state, so sizes of landed fish will also vary.  For example, the current minimum size is 12" in Massachusetts but 11" Maryland and Virginia.  Maryland,  Virginia, and Delaware manage with IFQs, not possession limits.  NC has relatively large possession limits (4000 or 3000 lbs), while the RI possession limit was 1,500lbs per week for the summer and typically around 100 lbs/day at other times in 2024. These regulations have implications for the distances that fishing firms can travel and still make a profit.


## Catching, Selling, and Classifying Fish

Black sea bass are caught by fishing vessels that typically use bottom trawl and pot/trap gear; gillnets and lines are less commonly used. Firms can target both sizes of fish and their catch of other species, but cannot perfectly control what they catch and land.  For example, the seasonal inshore movement during the spring and offshore movement affects things.  When fish offshore, they are less available to smaller vessels or those that are registered in states that manage with stringent possession limits.  Strong year classes can make a size class more available during certain years.   

After landing, fish are delivered to buyers (fish dealer), where they classified into market categories by the *dealer*.  There are four prevalent market categories (Jumbo, Large, Medium/Market, and Small), plus an "Unclassified" category^[There is a "Mixed or Unsized" market category (combined with the "Unclassified" market category). The Extra Small and PeeWee are combined with the Small category.]  Fish in the "Unclassified" or "Mixed" category are precisely that, a combination of other market categories. The definitions of the size classes may vary over time (year-to-year, month-to-month) or space (state-to-state or stock area to stock area).

It is quite well known that prices of fish depend on the body size, the fact consumers typically prefer larger fish motivates and necessitates the collection of size information by market category.  Prices are also influenced by other attributes, including freshness, fat content, water content,buyer- and seller- effects, and the quantity attributes supplied to the market^[@McConnell2000, @Carroll2001, @Kristofersson2004, @Kristofersson2007,  @Hammarlund2015,  @Gobillon2017, and @Ardini2018 are just a few examples.] 

Using prices to predict market category would only be possible if prices did vary by market category. A simple hedonic model confirms that they do.  The price of Jumbo Black Sea Bass is approximately \$6 per pound, while the price of small Black Sea Bass is closer to \$3.50 (Table \ref{HedonicTableA}).  Interestingly, the Unclassified market category has an average price of \$4.40 per pound, in between that of Large and Medium fish.^[A hedonic price model is not the main focus of this research, nevertheless, we report more details about the simple model in the Appendix.] 

\input{../results/hedonic_tableA.tex}

# Methods




## A Machine Learning Approach to Classification

Can prices be used to classify the "Mixed" or "Unclassifieds" into a standard market category? 

For all models, predictors (right hand side variables) would probably include price, year, month, state, gear, stockarea, stockunit, and some other things.

I'm not sure if it's better to split the data into Stock Units or to add stock units as a predictor.

The dealers are the ones that classify the fish. At minimum, we need to assume there is a dealer propensity to grade into particular classes.

The core of this research question is accurate prediction of the market category, not understanding the causal mechanisms by which one variable affects another.  Machine learning, which focuses on $\hat{y}$ instead of $\hat{\beta}$ is likely to be a good approach [@Varian2014, @Mullainathan2017].  Economic theory and content expertise can play a very important crucial role in starting the algorithm [@Mullainathan2017].

Train and Test a multinomial classification model  
  - Predict whether a transaction is Small, Medium, Large, or Jumbo classes^[Optionally train a separate model that admits the Unclassified, but I'm struggling to figure out what I learn from that].
       - Use either the multinomial log loss to do model training and selection.  
  - Validate in the usual way (weight of fish correctly predicted -- not the 'rows' of observations correctly predicted).
  -  

  A second, less-conventional validation might be available.  We have some years (2018-2020) where we sample lengths for the unclassified fish.  This can be used to construct a true catch-at-length distribution (outside the ML model). Inside the ML model, we predict the class of the "unclassified" into the constituent market categories and apply the length keys to those to form a catch-at-length distribution.  Construct some sort of loss function based on the differences in weights-at-length  (or weights at age). This assumes that the length measurements of the Unclassified category is "good."
    
  - Predict out of sample on the years where we do not have any lengths of Unclassified fish (2021-2023).

There are *many* machine learning models for multinomial classification.  The approach and following description draws heavily on [ @Strobl2009, @Varian2014, @Mullainathan2017, and @Hastie2023].  Because so much of the scientific enterprise at NEFSC is built on R, we look for relatively mature (old) methods that have been implemented in R. The general approach is:

1. Split the dataset into a "training" and "validation" datset.  Do nothing with the validation dataset until step 3. 
2. Further split the training dataset into k-folds to find the best hyperparameters (or tuning parameters -- these are essentially analyst-defined arguments to a machine learning routine that can be found by runing the ml routine many times, but not from 1 run of the model).
3. Run the machine learning model with the best hyperparameters on the full training dataset.
4. Predict on the validation dataset to demonstrate that the model fits well.
5. Predict out of sample.


Classification and Regression Trees [CART, @Breiman1984] and their evolutionary offshoots are attractive methods. A classification tree does recursive binary splitting.  The data is partitioned based on one explanatory variable and data in the same partition are given the same predicted value. Where to split is based on minimizing a measure of variation among member of the partition; a split is good if it results in all the members of the partition having the same value of the dependent variable.  The CART algorithm repeats this process for all partitions until a user specified stopping-criteria is met. It is top-down, because it begins operation on the whole dataset and it is "greedy" or "naive" because it does not look ahead. 

Classification trees are quite unstable and therefore can be prone to poor out of sample performance.  Bagging is a way to improve performance. Bagged classification trees simply bootstraps the classification tree and then aggregate in a reasonable way. Random forests are another improvement.  Random forests are bagged classification trees that also have a restricted the number of predictors. This increases the variation in the trees and can improve out of sample performance.

Classification trees and related methods have some problems with being biased toward selecting certain kinds of variables (factors with many factors, numerics, and variables with lots of missing values) [@Hothorn2006, @Strobl2009]. A criticism of both trees and random forests is that statistical properties don't guide the stopping criteria [@Hothorn2006, @Strobl2009]. A conditional inference process can address both of these problems [@Hothorn2006].


Dealer specific effects.  How do we handle the fact that dealers have their own tendencies in terms of how they classify fish? This needs to be handled this properly in the $k-$fold cross validation, otherwise the training and validation datasets are not truly independent and the model fit measures are overly optimistic.  


The model selection metric I use should be related to that.  For example, "True" landings in each market category compared to "actual" landings in each market category, computed at the stockarea-year, stockarea-semester, or stockarea-year-semester level.  


# Data

Matched dealer and vessel trip report data from 1996-2024 is available.  Vessel trip report data are trip-level reports of fishing activity by federally permitted vessels which include area fished, gear used, trip length, and estimates of quantity kept and discarded at the species level. Dealer data are transaction-level entries of sales by federally permitted fish dealers, by market category, from each vessel to each dealer. Matching is performed based on the Vessel trip report serial number. When this match fails, matching is done based on the vessel permit number, dealer identifier, and date which are contained in both the vessel reports and the dealer reports.

  - VTRs are sub-trip.
  _ We cannot match the dealer landings to a subtrip, so if a vessel fished in multiple statistical areas or used multiple gears, it's we have some measurement error here.

There are two empirical challenges.  First, what to do with these "multi-subtrip" transactions that do not match? Second, what to do with aggregates? 

Black Sea Bass: sales by firms without a federal permit to dealers without a federal permit.  In some states, firms without a federal permit can fish for black sea bass in state waters and sell it to fish dealers who do not have a federal permit. These states require vessels to report their catch at a lower frequency and catch from these vessels is added into the dealer data at a higher.

market_desc

dlrid,camsid ID variable
gear
price
,stockarea
, state
, year
, month
, semester
, lndlb
, grade_desc

For observation i, daily Landings by other vessels in the same state , by market category  
StateOtherQJumbo
StateOtherQLarge
StateOtherQMedium
StateOtherQSmall

For observation i, daily Landings by other vessels from the same stockarea , by market category  

StockareaOtherQJumbo
StockareaOtherQLarge
StockareaOtherQMedium
StockareaOtherQSmall

For observation i, 7 day sum of landings by vessels from the same stockarea, by market category

MA7_StockareaQJumbo
MA7_StockareaQLarge
MA7_StockareaQMedium
MA7_StockareaQSmall

For observation i, 7 day sum of landings by vessels from the same state, by market category
MA7_StateQJumbo
MA7_StateQLarge
MA7_StateQMedium
MA7_StateQSmall
MA7_stockarea_trips
MA7_state_trips

could probably add a gearcategory variable.

Dealer share of purchases by market category from 2013-2017   
need to see if it's okay to leave in a less than full rank set of predictors. shares sum to 1. 

ShareJumbo, ShareLarge, ShareMedium,ShareSmall, ShareUnclassified 

Dealer total transactions by market category from 2013-2017   
TransactionCountJumbo, TransactionCountLarge, TransactionCountMedium,TransactionCountSmall, TransactionCountUnclassified




## Random Forest 

Here is the call to the BSB classification recipe that defines the outcome, predictors, identification variables.  Not shown is the command that sets the ``case_weights=frequency_weights()`` using landed pounds for a row of data. Price and lndlb are the only continuous variables, all the others are factors. The dataset does not have any rows where ``market_desc==Unclassified`` 

```{r BSB_data, eval=F, echo=T}

# Use a recipe on the training data. 

# assign roles to predictors, outcome, groups, and weights
BSB.Classification.Recipe <- recipe(train_data) %>%
  update_role(market_desc, new_role = "outcome")%>%
  update_role(c(dlrid,camsid), new_role = "ID variable") %>%
  update_role(c(mygear,price,stockarea, state, year, month, semester, lndlb, grade_desc, trip_level_BSB), new_role = "predictor")

# State-level daily Landings on "other" trips, by market category  
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(StateOtherQJumbo, StateOtherQLarge, StateOtherQMedium, StateOtherQSmall), new_role = "predictor") 

# stockarea-level daily Landings on "other" trips, by market category  
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(StockareaOtherQJumbo, StockareaOtherQLarge, StockareaOtherQMedium, StockareaOtherQSmall), new_role = "predictor") 

# Trailing 7 days landings, by stockarea and market category   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_StockareaQJumbo, MA7_StockareaQLarge, MA7_StockareaQMedium, MA7_StockareaQSmall), new_role = "predictor")

# Trailing 7 days landings, by state and market category   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_StateQJumbo, MA7_StateQLarge, MA7_StateQMedium, MA7_StateQSmall), new_role = "predictor") 

# Trailing 7 day trips, by state and stock area.   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_stockarea_trips, MA7_state_trips), new_role = "predictor") 

# Dealer share of landings by market category from 2013-2017   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(ShareJumbo, ShareLarge, ShareMedium,ShareSmall, ShareUnclassified), new_role = "predictor") 

# Dealer transaction count of landings by market category from 2013-2017   

BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(TransactionCountJumbo, TransactionCountLarge, TransactionCountMedium,TransactionCountSmall, TransactionCountUnclassified), new_role = "predictor") 

# You can't center the factor variables
# rescale and recenter 
BSB.Classification.Recipe <- BSB.Classification.Recipe %>% 
#  step_impute_knn(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

recipe_summary<-BSB.Classification.Recipe %>% summary()

recipe_summary


```

I use the ``dlrid`` to control the way the k-fold cross validation is performed.  

Other predictors that could be available
  1. CAMS derived data
      A. STATUS, although it's probably better to put in "state permit" or "no federal permit"
  2. ``dlrid`` and or permit/vessel id.
  3. Landings of other things (that are caught along with particular market categories?). Like, do we tend to catch lots of smalls when also catching squid or summer flounder?
  4. Mesh size?  Or an interaction of mesh size and gear?  I'm not sure how to construct this for something like "other" or "hook" gears.
  5. Trip length. You might go farther to catch larger fish.

The Status==PZERO observations will have most of the typical "vtr" things missing (landings of other species, mesh size, trip length). 


Probably going to need to move this into the Rcontainer.


# Results


```{r child=here("results","mnl_logit.Rmd"), eval=FALSE}
```



# Machine Learning Results

## Goodness of Fit

## Notes



<!---
\newpage
--->
# References
<div id="refs"></div>



\clearpage

# Appendix{-}

```{r child=here("writing","Appendix_Hedonic.Rmd"), eval=TRUE}
```



