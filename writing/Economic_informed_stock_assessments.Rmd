---
title: 'Economic Informed Stock Assessments: Using Prices to Understand the Length of Fish'
author: "Min-Yang Lee and Emily Liljestrand"
date: "`r format(Sys.time(), '%B %d, %Y')`" 
documentclass: article
csl: "../ajae_mod.csl"
abstract: "The length- and age-structure of commercial landings is a crucial input to fish stock assessment. In the Northeast United States, NOAA Fisheries deploys samplers to fish dealers to measure the length of commercially landed fish.  Gaps in the data can arise when categories of certain fish simply are not sampled.  When this occurs, the information contained in fish prices can be used to fill in information about the size of these unmeasured fish. The size of individual fish is a characteristic than determines the price of fish; larger specimens nearly always receive higher prices than smaller ones. I use the Black Sea Bass fishery in the Northeast United States to illustrate my approach to using economic data to inform stock assessments.  There are 5 prevailing market categories: Jumbo, Large, Medium, Small, and Unclassified.  From 2020 to 2023, 5 to 10% of commercial landings were in the “Unclassified” market category; but no fish in this category were measured. I estimate a hedonic model to confirm that individual size impacts fish prices: “Unclassified” Black Sea Bass are priced between “Large” and “Medium” fish. Then I estimate a multinomial logit that explains market category as a function of prices and other attributes.  I use the results to predict the probability of the “Unclassified” belonging to any of the other market categories, allowing me to construct a price-informed length structure of commercial removals. "
keywords: "Hedonic Prices, Age structure, Stock assessment"
output:
  pdf_document: 
    includes:
      in_header: "../preamble-latex.tex"
    keep_tex: yes
    pandoc_args: --pdf-engine=pdflatex
    number_sections: true
  word_document: null
  html_document: null
fontsize: 12pt
bibliography: "C:/Users/min-yang.lee/Documents/library.bib"
---


 <!---- 
 The hardcoded bibilography is ghastly. 
Summary and Housekeeping

I did my data processing with  ``\stata_code\data_extraction_processing\extraction\commercial\00_cams_extraction.do`` and ``00_extraction_wrapper.do``

I also ran many of the  files in ``\stata_code\data_extraction_processing\analysis`` to do some exploratory analysis. 

Warnings: There is something funky going on with VA "status=PZERO" starting in 2021.  There is also something funky going on with Delaware landings, but there are no hullids/permit numbers so I don't think this will throw anything off at the vessel level.

--->

```{r global_options, include=FALSE}

 library("foreign")
 library("here")
 library("tidyverse")
 library("scales")
 library("knitr")
 library("lubridate")
 library("kableExtra")
 library("haven")
 library("mapview")
 library("sf")
library("htmltools")
here::i_am("writing/Economic_informed_stock_assessments.Rmd")

#############################################################################
#knitr options

knitr::opts_chunk$set(echo=FALSE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)


#############################################################################
my_images<-here("images")

descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")


# Read in statistical areas
stat_areas_location<-here("data_folder","external","shapefiles","Statistical_Areas_2010.shp")
stat_areas <- st_read(dsn = stat_areas_location)

#trim out some stat areas that I don't care about.
stat_areas<-stat_areas %>%
  dplyr::filter(Id>=460) %>% # Canada
   dplyr::filter(Id<=711) %>% # SERO
  dplyr::filter(!Id %in% c(640,650,660,670,680) ) #Offshore

```

# Introduction

<!--- Include the next 2 child documents.   --->
```{r, child=here("writing", c("summary.Rmd")),  eval=FALSE}
```

Can we use prices to predict market category?  

Can we "take it to production" for other stocks?  This requires something somewhat off the shelf methods and data that is regularly collected.


# Background

<!--- Include the next 2 child documents.   --->
```{r, child=here("writing", c("BSB_history.Rmd", "BSB_economic_background.Rmd")),  eval=FALSE}
```

## Black Sea Bass and the Stock Assessment Enterprise 

The age-structure of commercial landings is a crucial input to the stock assessment. Samplers measure the length of commercially landed fish and collect otoliths.  Analysts request coverage levels of different market categories. Gaps in the data can arise when categories of certain fish  are not sampled.  When this occurs, the information contained in fish prices can be used to fill in information about the size of these unmeasured fish. 

Need to justify having these as explanatory variables (predictors).
Stock area
Semester


```{r borrowed_lengths, fig.show = "hold", out.width = "48%", fig.cap="Borrowed Lengths in Black Sea Bass",  fig.align = "center", echo=FALSE}
knitr::include_graphics(here("images","background",c("2023_BSB_UNIT_TOR2_commercialdata_slide8.jpg")))
```

Slide 8 from ``2023_BSB_UNIT_ppt_TOR2_commercialdata.pdf`` illustrates the problem generally.  Pink in the bar graphs are good, this means there were enough measured fish in a market category.  Everything else indicates a certain amount of "borrowing" of data, where lengths are taken from market-area-semester combination.  

The underlying data is taken from **STOCKEFF**iciency, which originally used CFDBS and other sources. For more recent data, it points at "CAMS.CFDBS" style tables, which relies on CAMS.  It also does the area allocation also. It does some intermediate aggregation.  Then Emily pulls it from the stockeff webpage.  To bring anything to production probably will require pushing something to oracle.

For our purposes, we do not care about predicting a single "unclassified" observation well.  The research question is related to an aggregate of predictions: landings aggregated over a stockarea-year-semester (or perhaps stockarea-year).  

## Regulatory Environment

[only the details needed to establish why we have certain RHS variables in the the model]

State -- regulations vary by state, so sizes of landed fish will also vary.  For example, the current minimum size is 12" in Massachusetts but 11" Maryland and Virginia.  Maryland,  Virginia, and Delaware manage with IFQs, not possession limits.  NC has relatively large possession limits (4000 or 3000 lbs), while the RI possession limit was 1,500lbs per week for the summer and typically around 100 lbs/day at other times in 2024. These regulations have implications for the distances that fishing firms can travel and still make a profit.


## Catching, Selling, and Classifying Fish

Black sea bass are caught by fishing vessels that typically use bottom trawl and pot/trap gear; gillnets and lines are less commonly used. Firms can target both sizes of fish and their catch of other species, but cannot perfectly control what they catch and land.  For example, the seasonal inshore movement during the spring and offshore movement affects things.  When fish offshore, they are less available to smaller vessels or those that are registered in states that manage with stringent possession limits.  Strong year classes can make a size class more available during certain years.   

After landing, fish are delivered to buyers (fish dealer), where they classified into market categories by the *dealer*.  There are four prevalent market categories (Jumbo, Large, Medium/Market, and Small), plus an "Unclassified" category^[There is a "Mixed or Unsized" market category (combined with the "Unclassified" market category). The Extra Small and PeeWee are combined with the Small category.]  Fish in the "Unclassified" or "Mixed" category are precisely that, an unobserved combination of the other market categories. The definitions of the size classes may vary over time (year-to-year, month-to-month) or space (state-to-state or stock area to stock area).  Fish dealers may act as middlemen, simply selling fish to processors, restaurants, or final consumers.  They may do some processing themselves and sell filleted fish to entities farther along the value chain.

Prices of fish depend on the size of fish; the fact consumers typically prefer larger fish both motivates and necessitates the collection of size information by disaggregated by market category.  Prices are also influenced by other attributes, including freshness, fat content, water content,buyer- and seller- effects, and the quantity attributes supplied to the market^[@McConnell2000, @Carroll2001, @Kristofersson2004, @Kristofersson2007,  @Hammarlund2015,  @Gobillon2017, and @Ardini2018 are just a few examples.] 

Using prices to predict market category would only be possible if prices did vary by market category. A simple hedonic model confirms that they do.  The price of Jumbo Black Sea Bass is approximately \$6 per pound, while the price of small Black Sea Bass is closer to \$3.50 (Table \ref{HedonicTableA}).  Interestingly, the Unclassified market category has an average price of \$4.40 per pound, in between that of Large and Medium fish, suggesting that Unclassified fish may be a mix of these other two categories. ^[A hedonic price model is not the main focus of this research, nevertheless, we report more details about the simple model in the Appendix.] 

\input{../results/hedonic_tableA.tex}

# Methods and Data

The core of this research question is accurate prediction of the market category, not understanding the causal mechanisms by which one variable affects another.  Machine learning, which focuses on $\hat{y}$ instead of $\hat{\beta}$ is likely to be a good approach [@Varian2014, @Mullainathan2017].  Economic theory and on-the-ground expertise of the subject matter (the black sea bass fishery) helps guide variable selection [@Mullainathan2017].  We first provide a general description of the machine learning method, before describing and justifying the methods, modeling choices, and data that we use in this application.

Many machine learning models have been developed to perform multinomial classification.  The following description draws heavily on [@Strobl2009, @Varian2014, @Mullainathan2017, and @Hastie2023].  Classification and Regression Trees [CART, @Breiman1984] and their evolutionary offshoots are attractive methods for multinomial classification.  A classification tree performs recursive binary splitting.  The dependent variable is partitioned into two groups based on a single explanatory variable.  The optimal explanatory variable to use (and, for continuous explanatory variables, the value at which the split occurs) is based on minimizing a measure of variation among member of the partition.  A good split will results in a large reduction in the variation among the observations in the partition.  The CART algorithm continues splitting until a user specified stopping-criteria is met. It is top-down, because it begins operation on the whole dataset and it is "greedy" or "naive" because it does not look ahead. 

Classification trees are quite unstable and therefore can be prone to poor out of sample performance.  Bagging is a way to improve performance. Bagged classification trees simply bootstraps the classification tree and then aggregate in a reasonable way. Random forests are another improvement.  Random forests are bagged classification trees that also have a restricted the number of predictors. This increases the variation in the trees and can improve out of sample performance.

Training
  - Tuning hyper parameters
  - Training - after finding the best hyper parameters, fitting a final model
  - Validation -- out of sample prediction


## Our approach

We train a multinomial Random Forest 2018-2024 black sea bass sales from the Small, Medium, Large, and Jumbo market categories.  We use the  multinomial log-loss to do model tuning.  We use 80% of our dataset to train and tune the model, further dividing this into 10 folds for selecting optimal hyperparameters.  After tuning, we train the model on the entire 80% training sample and validate on the remaining 20\%. We use the finalized model to perform out-of-sample prediction to recover the true but unobserved combination of the other market categories.  Finally, we aggregate up to construct the new length structures of caught fish at the stockarea-semester level. 

Dealers are the entities that classify the fish and dealers are likely to have an unobserved propensity to grade fish into particular classes.  This poses minor problems for We cluster-sample during the tuning process; all observations from a single dealer are allocated to exactly one fold.  This ensures that the validation fold does not inadvertently contain "training data", which would lead to overly optimistic model fit.  Including a full set of binary indicator variables to account dealer-specific effects ("one-hot" encoding) can lead to both overfitting [@Breiman2001; @Hastie2023] and bias [c.f @Strobl2007; @Strobl2009]. Instead we use a variation of target encoding.

<!---  

need to blend in the data to make this a little easier to follow.
--->

<!---  

  A second, less-conventional validation might be available.  We have some years (2018-2020) where we sample lengths for the unclassified fish.  This can be used to construct a true catch-at-length distribution (outside the ML model). Inside the ML model, we predict the class of the "unclassified" into the constituent market categories and apply the length keys to those to form a catch-at-length distribution.  Construct some sort of loss function based on the differences in weights-at-length  (or weights at age). This assumes that the length measurements of the Unclassified category is "good."
  
  The model selection metric I use should be related to that.  For example, "True" landings in each market category compared to "actual" landings in each market category, computed at the stockarea-year, stockarea-semester, or stockarea-year-semester level.  
--->    


## Data

Matched dealer and vessel trip report data from 1996-2024 is available.  Vessel trip report data are trip-level reports of fishing activity by federally permitted vessels which include area fished, gear used, trip length, and estimates of quantity kept and discarded at the species level. Dealer data are transaction-level entries of sales by federally permitted fish dealers, by market category, from each vessel to each dealer. Matching is performed based on the Vessel trip report serial number. When this match fails, matching is done based on the vessel permit number, dealer identifier, and date which are contained in both the vessel reports and the dealer reports.

  - VTRs are sub-trip.
  _ We cannot match the dealer landings to a subtrip, so if a vessel fished in multiple statistical areas or used multiple gears, it's we have some measurement error here.

There are two empirical challenges.  First, what to do with these "multi-subtrip" transactions that do not match? Second, what to do with aggregates? 

Black Sea Bass: sales by firms without a federal permit to dealers without a federal permit.  In some states, firms without a federal permit can fish for black sea bass in state waters and sell it to fish dealers who do not have a federal permit. These states require vessels to report their catch at a lower frequency and catch from these vessels is added into the dealer data at a higher.

outcome variable: market_desc
id variables: dlrid,camsid 
case (frequency) weights: weighting (pounds sold)

There are 38 predictors:
price (nominal, I should try real)
gear
,stockarea
, state
, year
, month
, semester
, lndlb
, grade_desc

For observation i, daily Landings by other vessels in the same state , by market category  
StateOtherQJumbo
StateOtherQLarge
StateOtherQMedium
StateOtherQSmall

For observation i, daily Landings by other vessels from the same stockarea , by market category  

StockareaOtherQJumbo
StockareaOtherQLarge
StockareaOtherQMedium
StockareaOtherQSmall

For observation i, 7 day sum of landings by vessels from the same stockarea, by market category

MA7_StockareaQJumbo
MA7_StockareaQLarge
MA7_StockareaQMedium
MA7_StockareaQSmall

For observation i, 7 day sum of landings by vessels from the same state, by market category
MA7_StateQJumbo
MA7_StateQLarge
MA7_StateQMedium
MA7_StateQSmall
MA7_stockarea_trips
MA7_state_trips

could probably add a gearcategory variable.

Dealer share of purchases by market category from 2010-2014   
need to see if it's okay to leave in a less than full rank set of predictors. shares sum to 1. 

ShareJumbo, ShareLarge, ShareMedium,ShareSmall, ShareUnclassified 

Dealer total transactions by market category from 2010-2014   
TransactionCountJumbo, TransactionCountLarge, TransactionCountMedium,TransactionCountSmall, TransactionCountUnclassified




## Random Forest 

Here is the call to the BSB classification recipe that defines the outcome, predictors, identification variables.  Not shown is the command that sets the ``case_weights=frequency_weights()`` using landed pounds for a row of data. Price and lndlb are the only continuous variables, all the others are factors. The dataset does not have any rows where ``market_desc==Unclassified`` 

```{r BSB_data, eval=F, echo=T}

# Use a recipe on the training data. 

# assign roles to predictors, outcome, groups, and weights
BSB.Classification.Recipe <- recipe(train_data) %>%
  update_role(market_desc, new_role = "outcome")%>%
  update_role(c(dlrid,camsid), new_role = "ID variable") %>%
  update_role(c(mygear,price,stockarea, state, year, month, semester, lndlb, grade_desc, trip_level_BSB), new_role = "predictor")

# State-level daily Landings on "other" trips, by market category  
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(StateOtherQJumbo, StateOtherQLarge, StateOtherQMedium, StateOtherQSmall), new_role = "predictor") 

# stockarea-level daily Landings on "other" trips, by market category  
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(StockareaOtherQJumbo, StockareaOtherQLarge, StockareaOtherQMedium, StockareaOtherQSmall), new_role = "predictor") 

# Trailing 7 days landings, by stockarea and market category   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_StockareaQJumbo, MA7_StockareaQLarge, MA7_StockareaQMedium, MA7_StockareaQSmall), new_role = "predictor")

# Trailing 7 days landings, by state and market category   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_StateQJumbo, MA7_StateQLarge, MA7_StateQMedium, MA7_StateQSmall), new_role = "predictor") 

# Trailing 7 day trips, by state and stock area.   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(MA7_stockarea_trips, MA7_state_trips), new_role = "predictor") 

# Dealer share of landings by market category from 2013-2017   
BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(ShareJumbo, ShareLarge, ShareMedium,ShareSmall, ShareUnclassified), new_role = "predictor") 

# Dealer transaction count of landings by market category from 2013-2017   

BSB.Classification.Recipe <-BSB.Classification.Recipe %>%
  update_role(c(TransactionCountJumbo, TransactionCountLarge, TransactionCountMedium,TransactionCountSmall, TransactionCountUnclassified), new_role = "predictor") 

# You can't center the factor variables
# rescale and recenter 
BSB.Classification.Recipe <- BSB.Classification.Recipe %>% 
#  step_impute_knn(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

recipe_summary<-BSB.Classification.Recipe %>% summary()

recipe_summary


```

I use the ``dlrid`` to control the way the k-fold cross validation is performed.  

Other predictors that could be available
  1. CAMS derived data
      A. STATUS, although it's probably better to put in "state permit" or "no federal permit"
  2. ``dlrid`` and or permit/vessel id.
  3. Landings of other things (that are caught along with particular market categories?). Like, do we tend to catch lots of smalls when also catching squid or summer flounder?
  4. Mesh size?  Or an interaction of mesh size and gear?  I'm not sure how to construct this for something like "other" or "hook" gears.
  5. Trip length. You might go farther to catch larger fish.

The Status==PZERO observations will have most of the typical "vtr" things missing (landings of other species, mesh size, trip length). 


Probably going to need to move this into the Rcontainer.


# Results


```{r child=here("results","mnl_logit.Rmd"), eval=FALSE}
```



# Machine Learning Results

## Goodness of Fit

## Notes



<!---
\newpage
--->
# References
<div id="refs"></div>



\clearpage

# Appendix{-}

```{r child=here("writing","Appendix_Hedonic.Rmd"), eval=TRUE}
```



