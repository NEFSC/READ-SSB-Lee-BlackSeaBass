---
title: 'Economic Informed Stock Assessments: Using Prices to Understand the Length of Fish'
author: "Min-Yang Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`" 
documentclass: article
csl: "../ajae_mod.csl"
abstract: "The length- and age-structure of commercial landings is a crucial input to fish stock assessment. In the Northeast United States, NOAA Fisheries deploys samplers to fish dealers to measure the length of commercially landed fish.  Gaps in the data can arise when categories of certain fish simply are not sampled.  When this occurs, the information contained in fish prices can be used to fill in information about the size of these unmeasured fish. The size of individual fish is a characteristic than determines the price of fish; larger specimens nearly always receive higher prices than smaller ones. I use the Black Sea Bass fishery in the Northeast United States to illustrate my approach to using economic data to inform stock assessments.  There are 5 prevailing market categories: Jumbo, Large, Medium, Small, and Unclassified.  From 2020 to 2023, 5 to 10% of commercial landings were in the “Unclassified” market category; but no fish in this category were measured. I estimate a hedonic model to confirm that individual size impacts fish prices: “Unclassified” Black Sea Bass are priced between “Large” and “Medium” fish. Then I estimate a multinomial logit that explains market category as a function of prices and other attributes.  I use the results to predict the probability of the “Unclassified” belonging to any of the other market categories, allowing me to construct a price-informed length structure of commercial removals. "
keywords: "Hedonic Prices, Age structure, Stock assessment"
output:
  pdf_document: 
    includes:
      in_header: "../preamble-latex.tex"
    keep_tex: yes
    pandoc_args: --pdf-engine=pdflatex
    number_sections: true
  word_document: null
  html_document: null
fontsize: 12pt
bibliography: "../../library.bib"
---


 <!---- 
Summary and Housekeeping

I did my data processing with  ``\stata_code\data_extraction_processing\extraction\commercial\00_cams_extraction.do`` and ``00_extraction_wrapper.do``

I also ran many of the  files in ``\stata_code\data_extraction_processing\analysis`` to do some exploratory analysis. 

Warnings: There is something funky going on with VA "status=PZERO" starting in 2021.  There is also something funky going on with Delaware landings, but there are no hullids/permit numbers so I don't think this will throw anything off at the vessel level.

--->

```{r global_options, include=FALSE}

 library("foreign")
 library("here")
 library("tidyverse")
 library("scales")
 library("knitr")
 library("lubridate")
 library("kableExtra")
 library("haven")
 library("mapview")
 library("sf")
library("htmltools")
here::i_am("writing/BSB_data_summary.Rmd")

#############################################################################
#knitr options

knitr::opts_chunk$set(echo=FALSE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)


#############################################################################
my_images<-here("images")

descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")


# Read in statistical areas
stat_areas_location<-here("data_folder","external","shapefiles","Statistical_Areas_2010.shp")
stat_areas <- st_read(dsn = stat_areas_location)

#trim out some stat areas that I don't care about.
stat_areas<-stat_areas %>%
  dplyr::filter(Id>=460) %>% # Canada
   dplyr::filter(Id<=711) %>% # SERO
  dplyr::filter(!Id %in% c(640,650,660,670,680) ) #Offshore

```

# Introduction


# Summary: Research Question, Motivation, Extensions, and Potential Problems

## Research Question

Can inclusion of prices improve our ability to classify Black Sea Bass in the "Mixed" and "Unclassified" market categories into another market category?

## Motivation

Landed weights must be converted into landed numbers at age for age-structured stock assessments.  This is done by sampling landed fish at the dealer, vessel, or dock and measuring lengths and computing ages. When no or few fish in a market category are measured, a length distribution must be borrowed for the fish in the Unmeasured market categories.  For Black Sea bass, the unclassified market category was not measured for 3 years.

Because it is costlier to collect the age of fish, only a subset of the fish that have lengths also have ages. Once landings in a particular market category is converted to lengths, it is reasonable to apply a length-age relationship derived from 'all market categories' to just a single market category. Equivalently, it is reasonable to convert the landings from all market categories into a single weight-at-length measure and then convert that into an weight-at-age measure. 

The "Unclassified" market category, in conjunction with sampling at vessels, can be particularly challenging. Dealers, not vessels, classify fish into market categories, therefore sampling at the dock means that the "Unclassified" fish might not actually be unclassified. 

## Extensions

Can we use the methods to other fisheries? If it works well, can port sampling effort be re-deployed away from categories that are hard to get?

## Potential Questions and Problems

1. State-level aggregates.  There are rows in the dealer data that are state-level reports of sales by non-federal permits to non-federal dealers.  If these are transactions (trip/subtrip), they can be assigned as long as the value reported is the true value, not an imputed one.  If the transactions are aggregates, they *probably cannot* be assigned even if the true value reported.
  * I need to get in touch with the people that load in these state aggregates to figure out how prices are put into the datasets.
  * For now, I'm setting these outside the model.

2. Dealers have their own diverse tendencies in terms of how they classify fish.
  * This suggests that the dealer id should go into the model as a factor variable. 
  * It also suggest that cross-validation will be tricky.  The training and validation datasets would not be truly independent if I pull random samples.  See @Rabinowicz2022 and @Rabinowicz2022b for discussions in the context of a mixed linear model.

3. Is it reasonable to use "pounds landed" as a weighting variable and an explantory variable?
  * "A pound of landed fish" for weighting
  * A larger amount of landed (on a trip) is more likely to be graded instead of lumped as "unclassified." yikes, this is not a good justification. But this suggests trip-level (camsid) bsb landings
  * Targeting ---  landings of other species.
  
## Solved Problems

1.  Price premia: Prices need to vary by market category. If they there isn't any variation in prices across market categories, then it won't work. Prices don't need to have a particular relationship, and that relationship can probably change from year to year (or period-to-period). 

2.  Weighting observations. It is not reasonable to treat an observation of 10 lbs the same as an observation of 100lbs.  Using "pounds landed" as a frequency weighting seems appropriate.  This shifts the unit of observation to "a pound of landed fish," which more closely aligns with the goal of the model

3.  Overfitting.  The underlying problem is an out of sample $\hat{y}$ problem [@Mullainathan2017] -- I want to predict the probability of an observation belonging to particular class.  I don't care too much about inference.  

4.  Some of the RHS variables may be "sparse." This might make a k-fold cross-validation with a parametric estimator difficult.  If there's not enough variation, the maximum likelihood estimator can fail to converge. This should take some seriously bad luck, extreme sparsity, or a large number of folds.  This is a 

# Regulations and Background

<!--- Include the next 2 child documents.   --->
```{r, child=here("writing", c("BSB_history.Rmd", "BSB_economic_background.Rmd")),  eval=TRUE}
```


# Stock assessment

The age-structure of commercial landings is a crucial input to the stock assessment. Samplers measure the length of commercially landed fish and collect otoliths.  Analysts request coverage levels of different market categories. Gaps in the data can arise when categories of certain fish  are not sampled.  When this occurs, the information contained in fish prices can be used to fill in information about the size of these unmeasured fish. 

There are 5 prevailing market categories: Jumbo, Large, Medium, Small, and Unclassified.  Extra Small and PeeWee are aggregated into Small; these are very infrequent.  Mixed is aggregated into Unclassified.  From 2020 to 2023, 5 to 10% of commercial landings were in the “Unclassified” market category; but no fish in this category were measured.

Semester is Jan -June and July -Dec.

Gear doesn't matter for landed lengths, but yes for discards.


```{r borrowed_lengths, fig.show = "hold", out.width = "48%", fig.cap="Borrowed Lengths in Black Sea Bass",  fig.align = "center", echo=FALSE}
knitr::include_graphics(here("images","background",c("2023_BSB_UNIT_TOR2_commercialdata_slide8.jpg")))
```

Slide 8 from ``2023_BSB_UNIT_ppt_TOR2_commercialdata.pdf`` illustrates the problem generally.  Pink in the bar graphs are good, this means there were enough measured fish in a market category.  Everything else indicates a certain amount of "borrowing" of data, where lengths are taken from market-area-semester combination.  

The underlying data is taken from **STOCKEFF*iciency, which originally used CFDBS and other sources. For more recent data, it points at "CAMS.CFDBS" style tables, which relies on CAMS.  It also does the area allocation also. It does some intermediate aggregation.  Then Emily pulls it from the stockeff webpage.  To bring anything to production probably will require pushing something to oracle.


# Held assumptions.

1. Fish are caught before they are sold.
2. Fishing firms catch a vector of size classes (call it inches). Fish are sorted into market categories by the *dealer*. For black sea bass, there are 4 classes (Jumbo, Large, Medium/Market, and Small) plus an "Unclassified" or "Mixed" category. 
3. Firms can imperfectly adjust their output vector.The goal is always to maximize profits. 

  - Larger fish are available at certain times of the year.
  - Larger fish are available at certain parts of the ocean, which is correlated with state or trip length.
  - Larger fish are available to certain kinds of gear.
  - Larger fish are available during certain seasons, or during certain years.
4. Fish in the "Unclassified" or "Mixed" category are a combination of sizes of the other market categories.
  - They have a 'length', but we don't know what it is.
  - They could be disaggregated into their constituent sizes by the dealer; however it's not required and it takes a few extra minutes to type all that in.
  - State level reporting aggregates lots of observations
  - There is probably alot of dealer-specific effects here. Some dealers classify everything. Some classify nothing.

5. The definition of the size classes may vary over time (year-to-year, month-to-month) or space (state-to-state).

6. The research question is related to an aggregate of predictions: landings aggregated over a stockarea-year-semester (or perhaps stockarea-year).  The model selection metric I use should be related to that.  For example, "True" landings in each market category compared to "actual" landings in each market category, computed at the stockarea-year, stockarea-semester, or stockarea-year-semester level.  

# Methods

## Do prices vary by size? 

We first estimate a linear hedonic model in which the per-pound price of a lot of fish is a function of the attributes of that lot, including  market category, gear, and time of year. @McConnell2000 and @Carroll2001 are early examples of hedonic models applied to tuna prices: attributes like freshness, fat content, and individual size are associated with positive price premia.  @Kristofersson2004, @Kristofersson2007, and @Hammarlund2015 examine demand for attributes in a hierarchical linear model; finding that, for example, large fish receive higher prices but when the quantity supplied of large fish is higher than average, the price premia is smaller than average.  Other attributes can impact prices: @Gobillon2017 examine buyer- and seller- effects. 

The motivating concern is there are independent variables that are correlated with size could be impacting prices.  I don't think this is a major concern.

I've estimated this in stata using least squares with a handful of different specifications.

## Can prices be used to classify the "Mixed" or "Unclassifieds" into a normal market category? 

For all models, predictors (right hand side variables) would probably include price, year, month, state, gear, stockarea, stockunit, and some other things.

I'm not sure if it's better to split the data into Stock Units or to add stock units as a predictor.

The dealers are the ones that classify the fish. At minimum, we need to assume there is a dealer propensity to grade into particular classes.

### Classical Statistics Approach

Estimate a categorical model (multinomial logit) with price and other things as explanatory variables. 
  - Model selection and validate the model with a k-fold cross validation (?)
  - Predict out of sample for the unclassified market category
  - Validate again by predicting out of sample lengths for the unclassified fish and compare to the years where we do have lengths for the unclassified fish.
  - I've also estimated a multinomial logit in stata. Cross-validation and model selection is a little harder, but can be done by hand. 
  - Handling the dealer's "market category propensity" is a problem for inference, but a "fixed effects" logit might not be too awful for prediction.  A correlated random effects inspired approach, where we put in something like a cumulative fraction in each market category might work.  
  
Estimate a latent class (finite mixture model) on the price of unclassified, where there are a moderate number of classes (say small and large).  
  - Validate the model with a k-fold cross-validation.
  - Bonus validation: Validate the model by predicting categories for known small and large. 
  - Predict the class membership of the unclassified.  

### Machine Learning Approach

The core of this research question is accurate prediction of the market category, not understanding the causal mechanisms by which one variable affects another.  Machine learning, which focuses on $\hat{y}$ instead of $\hat{\beta}$ is likely to be a good approach [@Varian2014, @Mullainathan2017].  Economic theory and content expertise can play a very important crucial role in starting the algorithm [@Mullainathan2017].

Train and Test a multinomial classification model  
  - Predict whether a transaction is Small, Medium, Large, or Jumbo classes^[Optionally train a separate model that admits the Unclassified, but I'm struggling to figure out what I learn from that].
       - Use either the Brier classification or the log loss to do model training and selection.  
  - Validate in the usual way (weight of fish correctly predicted -- not the 'rows' of observations correctly predicted).
  -  

  A second, less-conventional validation might be available.  We have some years (2018-2020) where we sample lengths for the unclassified fish.  This can be used to construct a true catch-at-length distribution (outside the ML model). Inside the ML model, we predict the class of the "unclassified" into the constituent market categories and apply the length keys to those to form a catch-at-length distribution.  Construct some sort of loss function based on the differences in weights-at-length  (or weights at age). This assumes that the length measurements of the Unclassified category is "good."
    
  - Predict out of sample on the years where we do not have any lengths of Unclassified fish (2021-2023).

There are *many* machine learning models for multinomial classification.  The approach and following description draws heavily on [@Varian2014, @Mullainathan2017, @Hastie2023, and @Strobl2009].  Because so much of the scientific enterprise at NEFSC is built on R, we look for relatively mature (old) methods that have been implemented in R. The general approach is:

1. Split the dataset into a "training" and "validation" datset.  Do nothing with the validation dataset until step 3. 
2. Further split the training dataset into k-folds to find the best hyperparameters (or tuning parameters -- these are essentially analyst-defined arguments to a machine learning routine that can be found by runing the ml routine many times, but not from 1 run of the model).
3. Run the machine learning model with the best hyperparameters on the full training dataset.
4. Predict on the validation dataset to demonstrate that the model fits well.
5. Predict out of sample.


Classification and Regression Trees [CART, @Breiman1984] and their evolutionary offshoots are attractive methods. A classification tree does recursive binary splitting.  The data is partitioned based on one explanatory variable and data in the same partition are given the same predicted value. Where to split is based on minimizing a measure of variation among member of the partition; a split is good if it results in all the members of the partition having the same value of the dependent variable.  The CART algorithm repeats this process for all partitions until a user specified stopping-criteria is met. It is top-down, because it begins operation on the whole dataset and it is "greedy" or "naive" because it does not look ahead. 

Classification trees are quite unstable and therefore can be prone to poor out of sample performance.  Bagging is a way to improve performance. Bagged classification trees simply bootstraps the classification tree and then aggregate in a reasonable way. Random forests are another improvement.  Random forests are bagged classification trees that also have a restricted the number of predictors. This increases the variation in the trees and can improve out of sample performance.

Classification trees and related methods have some problems with being biased toward selecting certain kinds of variables (factors with many factors, numerics, and variables with lots of missing values) [@Hothorn2006, @Strobl2009]. A criticism of both trees and random forests is that statistical properties don't guide the stopping criteria [@Hothorn2006, @Strobl2009]. A conditional inference process can address both of these problems [@Hothorn2006].


Dealer specific effects.  How do we handle the fact that dealers have their own tendencies in terms of how they classify fish? This needs to be handled this properly in the $k-$fold cross validation, otherwise the training and validation datasets are not truly independent and the model fit measures are overly optimistic. However, we would also want to include dealer specific effects as predictors. See @Rabinowicz2022 and @Rabinowicz2022b discusses the applicability of cross-validation in the context of a mixed linear model.  There are two take-aways

@Rabinowicz2022b 's setup:
1. the data is indexed $i=1,...,N$ and broken up so that $i=1,\ldots,n_1$ are in the first fold, $i=n_1+1,\ldots,n_2$ are in the second fold, etc.
2. $T_{1}=\{y_i,\bf{x_i}\}_{i=1}^{n_1}$ is the collection of predictors and outcomes in the $1$st of the $K$ folds. The subscripting is a little difficult here.   $T_{2}=\{y_i,\bf{x_i}\}_{i=n_1+1}^{n_2}$ is the data in the 2nd fold, etc.
3. For a specific fold, the usual cross-validation error estimator an expected value of a loss function of the true outcome and the predictions made by combining the "out-of-fold" model with the data in in the fold. The fold-specific errors are averaged over each of the $K$ folds.  This is standard and the fold-specific loss function for observation $i$ in fold $k$ is written as:
$$
L(y_i, \hat{y}(x_i, T_{-k}))
$$
This captures the "true data" $y_i, \bf{x_i}$, and the model trained on the $-k$ folds of data.

3. A "more general" form of the Cross-Validation prediction error is the generalization error (hah) in Equation 2. It is the expectation of a modified loss function:
$$
L(y_{te,i}, \hat{y}(x_{te,i}, T_{tr}))
$$
This captures the "true data" $y_i, \bf{x_i}$, and the model trained on some generic data.  Averaging this loss function over "everything" produces the generalized error. When the data are uncorrelated, the standard formula for a CrossValidation error rate is an unbiased estimate of the generalization error.  This motivates the random fold process.  The standard formula is also an unbiased estimate when the data are "exchangeable" (Anderson 2018, Roberts 2017).  It is also unbiased for predicting new observations from the same clusters in the training dataset, which was not well understood by the literature.   

For certain other cases, a correction factor is needed which requires computing the covariance matrix between various predictions.  





# Data

Matched dealer and vessel trip report data from 1996-2024 is available.  Vessel trip report data are trip-level reports of fishing activity by federally permitted vessels which include area fished, gear used, trip length, and estimates of quantity kept and discarded at the species level. Dealer data are transaction-level entries of sales by federally permitted fish dealers, by market category, from each vessel to each dealer. Matching is performed based on the Vessel trip report serial number. When this match fails, matching is done based on the vessel permit number, dealer identifier, and date which are contained in both the vessel reports and the dealer reports.

  - VTRs are sub-trip.
  _ We cannot match the dealer landings to a subtrip, so if a vessel fished in multiple statistical areas or used multiple gears, it's we have some measurement error here.

There are two empirical challenges.  First, what to do with these "multi-subtrip" transactions that do not match? Second, what to do with aggregates? 

Black Sea Bass: sales by firms without a federal permit to dealers without a federal permit.  In some states, firms without a federal permit can fish for black sea bass in state waters and sell it to fish dealers who do not have a federal permit. These states require vessels to report their catch at a lower frequency and catch from these vessels is added into the dealer data at a higher.


## Random Forest 

Here is the call to the BSB classification recipe that defines the outcome, predictors, identification variables.  Not shown is the command that sets the ``case_weights=frequency_weights()`` using landed pounds for a row of data. Price and lndlb are the only continuous variables, all the others are factors. The dataset does not have any rows where ``market_desc==Unclassified`` 

```{r BSB_data, eval=F, echo=T}
BSB.Classification.Recipe <- recipe(train_data) %>%
  update_role(market_desc, new_role = "outcome")%>%
  update_role(c(dlrid,camsid), new_role = "ID variable") %>%
  update_role(c(mygear,price,stockarea, state, year, month, semester, lndlb, grade_desc), new_role = "predictor") %>% 
  update_role(c(StateOtherQJumbo, StateOtherQLarge, StateOtherQMedium, StateOtherQSmall), new_role = "predictor") %>% # State-level daily Landings on "other" trips, by market category  
  update_role(c(StockareaOtherQJumbo, StockareaOtherQLarge, StockareaOtherQMedium, StockareaOtherQSmall), new_role = "predictor") %>% # stockarea-level daily Landings on "other" trips, by market category  
  update_role(c(MA7_StockareaQJumbo, MA7_StockareaQLarge, MA7_StockareaQMedium, MA7_StockareaQSmall), new_role = "predictor") %>% # Trailing 7 days landings, by stockarea and market category   
  update_role(c(MA7_StateQJumbo, MA7_StateQLarge, MA7_StateQMedium, MA7_StateQSmall), new_role = "predictor") %>% # Trailing 7 days landings, by state and market category   
  update_role(c(MA7_stockarea_trips, MA7_state_trips), new_role = "predictor") # Trailing 7 day trips, by state and stock area.   

# You can't center the factor variables
# rescale and recenter 
BSB.Classification.Recipe <- BSB.Classification.Recipe %>% 
#  step_impute_knn(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

recipe_summary<-BSB.Classification.Recipe %>% summary()

recipe_summary


```

I use the ``dlrid`` to control the way the k-fold cross validation is performed.  

Other predictors that could be available
  1. CAMS derived data
      A. STATUS, although it's probably better to put in "state permit" or "no federal permit"
  2. ``dlrid`` and or permit/vessel id.
  3. Landings of other things (that are caught along with particular market categories?). Like, do we tend to catch lots of smalls when also catching squid or summer flounder?
  4. Mesh size?  Or an interaction of mesh size and gear?  I'm not sure how to construct this for something like "other" or "hook" gears.
  5. Trip length. You might go farther to catch larger fish.

The Status==PZERO observations will have most of the typical "vtr" things missing (landings of other species, mesh size, trip length). 


Probably going to need to move this into the Rcontainer.


# Results

## A Simple Hedonic Model
We first estimate a simple hedonic model using data from 2018-2024. We omit the "questionable observations" from VA and DE.  We aggregate to the "CAMSID, market-category, gear, area, datesail, dateland, dealer" level (this is basically the CAMSID-subtrip-market category"). We deflate to real prices using the CPI-U for Q1 of 2023.  We perform some aggregation of gears, aggregating to the following gear categories: "Line and Hand", "Trawl", "Gillnet", "Pot and Trap" and "all other" gears.  We aggregate the "Mixed or Unsized" market category with the "Unclassified" market category. We also aggregate the Small, Extra Small, and PeeWee into a single, Small category. Tables \ref{ESTtransactions}  and \ref{ESTavglbs}  summarize the number of observations and average landings per transaction for the estimation sample.

\input{../results/EST_transactions.tex}

\input{../results/EST_avg_lbs.tex}

We include transactions from New Hampshire to North Carolina where the real price is between \$0.15 and \$15 per pound. The quantity of fish in each transaction varies and an unweighted regression of price on independent variables would explain the conditional mean price per transaction. It is not reasonable to treat transactions of 2 lbs the same as transactions of 200 or 2,000 lbs. Therefore we weight each transaction by the pounds landed and the weighted regression can be interpreted as explaining the conditional mean price per pound.  Since the CAMS data "allocates" quantity sold to statistical areas or gears, this weighting makes the regression "invariant" to that allocation.  

Table \ref{HedonicTableA} contains some results of the more interesting results. There are 28.5 millions pounds of Black Sea Bass spread over 350,000 transactions during the 2018-2024 time period.  We prefer the Weighted specification, which has a more intuitive interpretation. Jumbo Black Sea bass recieved \$5.89 per pound, which Small Black Sea bass were prices at \$3.47.  Unclassified Black Sea bass were priced between the Medium and Large Market categories, suggesting that average size of  Unclassified fish lies between those two categories. All of the gear categories receive premia relative to trawl caught fish.  Live fish receives a slight premia relative to small fish. Increases in Total landings cause slight declines in prices, this occurs at a decreasing rate. 


Table \ref{HedonicTableB} shows the results for the month and state dummies. The Year dummies indicate a substantial decrease in the real price of black sea bass over the 2018-2024 period.  Prices have also fallen in nominal terms (not shown, Appendix if necessary).

<!---This will read a .tex file.  
 --->
\input{../results/hedonic_tableA.tex}

\input{../results/hedonic_tableB.tex}


<!---This will read a md table. but I can't get the figure numbering and captioning to work well.
```{r hedonictable, echo=FALSE, results='asis', caption="MY TABLE" }
res <- knitr::knit_child(here("results","hedonic_table.md"), quiet = TRUE)
cat(res, sep = '\n')
```
--->



<!--- This will render a markdown table. I don't know how to get the figure numbering and captions to work properly yet. It looks pretty good in html or in pdf. 
```{r child=here("results","hedonic_table.md"), caption="Test"}
```
--->


<!--- This will render a markdown table. I don't know how to get the figure numbering and captions to work properly yet.
```{r caption="TESTME"}
  htmltools::includeMarkdown(here("results","hedonic_table.md"))
```
 --->



## A Multinomial  Logit

A Multinomial logit is estimated on the four primary market categories (Jumbo, Large, Medium, and Small).  The omitted base category is the Large category. As with the hedonic model, I have weighted the rows by pounds landed and estimate on the "pooled" dataset with all years together.

$\begin{aligned}
Pr[Y=1| X] &= \frac{e^{X\beta^1}}{\sum_{j=1}^4 e^{X\beta^j}}\\
Pr[Y=2| X] &= \frac{e^{X\beta^2}}{\sum_{j=1}^4 e^{X\beta^j}}\\
Pr[Y=3| X] &= \frac{e^{X\beta^3}}{\sum_{j=1}^4 e^{X\beta^j}}\\
Pr[Y=4| X] &= \frac{e^{X\beta^4}}{\sum_{j=1}^4 e^{X\beta^j}}
\end{aligned}$

Following convention, one $j$ (Large) is chosen as a baseline and the corresponding $\beta^j$'s are set to zero to identify the other set of coefficients.

$\begin{aligned}
Pr[Y=1| X] &= \frac{e^{X\beta^1}}{\sum_{j=1}^3 e^{X\beta^j}}\\
Pr[Y=2| X] &= \frac{e^{X\beta^2}}{\sum_{j=1}^3 e^{X\beta^j}}\\
Pr[Y=3| X] &= \frac{e^{X\beta^3}}{\sum_{j=1}^3 e^{X\beta^j}}\\
Pr[Y=4| X] &= \frac{1}{\sum_{j=1}^Je^{X\beta^j}}
\end{aligned}$

The relative probability of Jumbo to the baseline is $e^{X\beta^1}$.  A 1 unit change in an element of $x_i$ changes the relative probability by:

$$\frac{e^{X_{\tilde i}\beta_{\tilde i}^1 + (x_i+1)\beta_i^1}}{e^{X\beta^1}}$$.  This is just $e^{beta_i^1}$. 

I've centered the price variable at the grand mean (\$2.84 per pound), included a full set of dummy variables for gear, and estimated the models without a constant. The gear RRR's can be interpreted as the relative risk of landings being in a particular category compared to the large baseline for an average priced transaction, landed in January of 2018 in Rhode Island.  Rhode Island was chosen as the baseline state because it had a mix of landings across the size classes (some states had very small amounts of landings in some categories).

Tables \ref{mlogitNomACentered} and \ref{mlogitNomBCentered} show the relative risk ratios for price, gear, and month. Values greater than 1 indicate that increases in an independent variable increase the probability that the transaction will be in a particular market category compared to the base category. 

Here is some interpretation:

1. Landings by Gillnets (in January 2018 in RI) have a 53\% risk of being Jumbo compared to Large.  They have a 111\% risk of being Medium and a 153\% risk of being small. 
2. Landings in October (of 2018 in RI) have a 72\% risk of being Jumbo compared to large.
3. Landings in MA have a much larger risk of being Jumbo compared to large, which landings in all the other states except VA have a larger risk of being Medium. RI is the most likely place for Smalls to be landed-- all the other State coefficients for Small are well below 1. 
4. 2019 had lots of Mediums, 2020 had lots of Jumbos. The other year coefficients are less than 1, indicating larges were the most prevalent.
5. The Price effect is really large.  A \$1 increase in price produces a 330\% change in relative risk ratio for Jumbo to Large. The same increase produces a very large decrease relative risk ratio of Medium to Large and an even greater one for Small to Large. 

<!---
\input{../results/mlogitA.tex}

\input{../results/mlogitB.tex}
--->

It looks like the yearly effects get smaller in magnitude when I estimate the nominal version. Still should check out a shift in within season or location? 
 
\input{../results/mlogitNomA_centered.tex}

\input{../results/mlogitNomB_centered.tex}

# Machine Learning Results

## Goodness of Fit

## Notes



<!---
\newpage
--->
# References
<div id="refs"></div>



\clearpage

# Appendix{-}

Tables \ref{FStransactions}  and \ref{FSavglbs}  summarize the number of observations and average landings per transaction for the full sample.

\input{../results/FS_transactions.tex}

\input{../results/FS_avg_lbs.tex}




