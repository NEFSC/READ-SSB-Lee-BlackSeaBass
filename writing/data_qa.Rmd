---
title: "Black Sea Bass: Estimation Datset QA"
author: "Min-Yang Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    fig_caption: yes
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    number_sections: yes
header-includes: \usepackage{setspace}\doublespacing
urlcolor: blue
editor_options:
  chunk_output_type: console
fontsize: 12pt
---

# Exploring Unclassifieds and CAMS status

<!---- 
**********************************************************************
* Purpose: 	Quality Assurance and checks on data for machine learning models.
* Inputs:
*   - BSB_estimation_dataset (from data_prep_ml.Rmd)
*   - BSB_unclassified_dataset (from data_prep_ml.Rmd)


* Outputs:

**********************************************************************


 The global_options chunk loads libraries, sets options, figures out if you're on a desktop or server, sets years, and sets graphing options
 --->
```{r global_options, include=FALSE}

library("here")

# load tidyverse and related
library("tidyverse")
library("scales")
library("glue")
# load tidyverse and related
library("tidymodels")
library("haven")
library("hardhat")
# load machine learning and estimation tools
# ranger imports RcppEigen and Rcpp, all 3 need to be compiled on unix.
# you might want to install Rcpp, then RcppEigen, then ranger

library("nnet")
library("ranger")

library("partykit")
library("bonsai")
# load utilities
library("knitr")
library("kableExtra")
library("viridis")
library("skimr")
library("conflicted")

#deal with conflicts
conflicts_prefer(dplyr::filter())
conflicts_prefer(dplyr::lag())
conflicts_prefer(purrr::discard())
conflicts_prefer(dplyr::group_rows())
conflicts_prefer(yardstick::spec())
conflicts_prefer(recipes::fixed())
conflicts_prefer(recipes::step())
conflicts_prefer(viridis::viridis_pal())

here::i_am("writing/data_qa.Rmd")

#traverse over to the DataPull repository
mega_dir<-dirname(here::here())
data_pull_dir<-file.path(mega_dir,"READ-SSB-Lee-BSB-DataPull")

#############################################################################
#knitr options

knitr::opts_chunk$set(echo=FALSE, warning = FALSE, error = FALSE, message = FALSE, comment = FALSE, cache = FALSE, progress = TRUE, verbose = FALSE, 
											dpi = 600)
options(tinytex.verbose = TRUE)
# options(knitr.table.format = "latex")
options(scipen=999)


lbs_per_mt<-2204.62
#############################################################################
my_images<-here("images")
descriptive_images<-here("images","descriptive")
exploratory_images<-here("images","exploratory")
vintage_string<-list.files(here("data_folder","main","commercial"), pattern=glob2rx("BSB_estimation_dataset*Rds"))
vintage_string<-gsub("BSB_estimation_dataset","",vintage_string)
vintage_string<-gsub(".Rds","",vintage_string)
vintage_string<-max(vintage_string)
estimation_vintage<-as.character(Sys.Date())






# Determine what platform the code is running on and set the number of threads for ranger
platform <- Sys.info()['sysname']
# check the name of the effective_user
if(platform == 'Linux'){
  if (grep("PREEMPT_DYNAMIC",Sys.info()['version'])==1){
      runClass<-'DynamicContainer'
    } else{ 
      runClass <- 'Container'
    }
  }

if(platform == 'Windows'){
  runClass<-'Windows'
}

if (runClass %in% c('Local', 'Windows')){
  my.ranger.threads<-6
} else if (runClass %in% c('Container')){ 
  my.ranger.threads<-8
}else if (runClass %in% c('DynamicContainer')){ 
  my.ranger.threads<-50

}



vintage_string<-list.files(here("data_folder","main","commercial"), pattern=glob2rx("BSB_estimation_dataset*Rds"))
vintage_string<-gsub("BSB_estimation_dataset","",vintage_string)
vintage_string<-gsub(".Rds","",vintage_string)
vintage_string<-max(vintage_string)
```

```{r load_in_data and results, include=FALSE}
# this was created with data_prep_ml.Rmd

cleaned_landings<-read_dta(here("data_folder","main","commercial", paste0("landings_cleaned_",vintage_string,".dta")))
camsid_specific_stats<-read_dta(here("data_folder","main","commercial", paste0("camsid_specific_cleaned_",vintage_string,".dta")))
daily_ma<-read_dta(here("data_folder","main","commercial", paste0("daily_ma_",vintage_string,".dta")))
state_ma<-read_dta(here("data_folder","main","commercial", paste0("state_ma_",vintage_string,".dta")))
stockarea_ma<-read_dta(here("data_folder","main","commercial", paste0("stockarea_ma_",vintage_string,".dta")))
dlrid_historical<-read_dta(here("data_folder","main","commercial", paste0("dlrid_historical_stats_",vintage_string,".dta")))
dlrid_lag<-read_dta(here("data_folder","main","commercial", paste0("dlrid_lag_stats_",vintage_string,".dta")))
gear_ma<-read_dta(here("data_folder","main","commercial", paste0("gear_ma_",vintage_string,".dta")))


estimation_dataset<-readr::read_rds(file=here("data_folder","main","commercial",paste0("BSB_estimation_dataset",vintage_string,".Rds")))
unclassified_dataset<-read_rds(file=here("data_folder","main","commercial",paste0("BSB_unclassified_dataset",vintage_string,".Rds")))
combined_dataset<-rbind(estimation_dataset,unclassified_dataset)

 # for reproducibility
 set.seed(4587315)

# construct the "case weights" variable here and trim out the extra factor levels from market_desc.
combined_dataset<-combined_dataset %>%
     mutate(weighting = frequency_weights(weighting),
            market_desc=fct_drop(market_desc))

keep_cols<-c("market_desc","dlrid","camsid","weighting", "mygear","price","priceR_CPI", "stockarea","state", "year","month", "semester","lndlb", "grade_desc", "trip_level_BSB")
keep_cols<-c(keep_cols,"StateOtherQJumbo", "StateOtherQLarge", "StateOtherQMedium", "StateOtherQSmall" )
keep_cols<-c(keep_cols,"StockareaOtherQJumbo", "StockareaOtherQLarge", "StockareaOtherQMedium", "StockareaOtherQSmall" )
keep_cols<-c(keep_cols,"MA7_StockareaQJumbo", "MA7_StockareaQLarge", "MA7_StockareaQMedium", "MA7_StockareaQSmall" )
keep_cols<-c(keep_cols,"MA7_StateQJumbo", "MA7_StateQLarge","MA7_StateQMedium", "MA7_StateQSmall")
keep_cols<-c(keep_cols,"MA7_stockarea_trips", "MA7_state_trips" )
keep_cols<-c(keep_cols,"Share2014Jumbo", "Share2014Large", "Share2014Medium","Share2014Small", "Share2014Unclassified" )
keep_cols<-c(keep_cols,"TransactionCountJumbo", "TransactionCountLarge", "TransactionCountMedium", "TransactionCountSmall", "TransactionCountUnclassified" )
keep_cols<-c(keep_cols,"LagSharePoundsJumbo","LagSharePoundsLarge", "LagSharePoundsMedium","LagSharePoundsSmall","LagSharePoundsUnclassified")
keep_cols<-c(keep_cols,"LagShareTransJumbo", "LagShareTransLarge", "LagShareTransMedium","LagShareTransSmall", "LagShareTransUnclassified")
#keep_cols<-c(keep_cols,"status" )

#combined_dataset<- combined_dataset %>%
#  select(all_of(keep_cols))



#Factor the cams status column
cleaned_landings<-cleaned_landings %>%
  mutate(status=factor(status,levels=c("MATCH","DLR_ORPHAN_SPECIES","DLR_ORPHAN_TRIP","PZERO"))
  )

cleaned_landings<-cleaned_landings %>%
  mutate(shore=as.numeric(hullid=="FROM_SHORE"),
         nofederal=as.numeric(str_detect(camsid, "^000000*"))
  )
```



# Read in start Data

Make dataset of landings, trips and landing per trip at the year and stock level. This is used to see how much gets dropped out at the end.

```{r landings and trips}

start_data<-cleaned_landings %>%
  group_by(stockarea,year, camsid) %>%
  summarise(lndlb=sum(lndlb)) %>%
  ungroup() %>%
  group_by(stockarea,year) %>%
  summarise(lnd_mt=sum(lndlb/2204),
            trips=n()) %>%
  mutate(lbs_per_trip=lnd_mt*2204/trips) %>%
  filter(year>=2015) %>%
  mutate(year=forcats::as_factor(year),
          stockarea=haven::as_factor(stockarea, levels="label")) 
```








# Data summaries

In the original "cleaned" dataset, I have about 1,100-1,300 trips per year in the South and 10-20x more trips in the North.

```{r initial_dataset}
knitr::kable(start_data, caption='Initial Dataset Landings and Trips by stockarea and year', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```


``` {r summarize_combined_dataset}

totals<-combined_dataset %>%
  group_by(stockarea,year, camsid) %>%
  summarise(lndlb=sum(lndlb)) %>%
  ungroup() %>%
  group_by(stockarea,year) %>%
  summarise(lnd_mt=sum(lndlb/2204),
            trips=n()) %>%
  mutate(lbs_per_trip=lnd_mt*2204/trips)

knitr::kable(totals, caption='Full Dataset Landings and Trips',format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```




``` {r summarize_estimation_dataset}

totalsE<-estimation_dataset %>%
  group_by(stockarea,year, camsid) %>%
  summarise(lndlb=sum(lndlb)) %>%
  ungroup() %>%
  group_by(stockarea,year) %>%
  summarise(lnd_mt=sum(lndlb/2204),
            trips=n()) %>%
  mutate(lbs_per_trip=lnd_mt*2204/trips)


knitr::kable(totalsE, caption='Four Principal Classes Landings and Trips',format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```

# How much data do I have

Is there "enough" data in just the South?  There are about 1,300 trips per year.

``` {r graph_combined_obs,fig.cap = "Number of observations by Market category and stockarea"}

totalsMC<-combined_dataset %>%
  group_by(stockarea,year, camsid, market_desc) %>%
  summarise(lndlb=sum(lndlb)) %>%
  ungroup() %>%
  group_by(stockarea,year,market_desc) %>%
  summarise(lnd_mt=sum(lndlb/2204),
            observations=n()) 



ggplot(totalsMC)+
aes(x=year, y=observations) +
  geom_point() +
  ylab("obs")+ 
    facet_grid(stockarea~market_desc, scales="free")

```



``` {r table_for_combined_obs}

obs<-totalsMC %>%
  select(-lnd_mt) %>%
  pivot_wider(values_from=observations, names_from=stockarea, names_glue="Observations_{stockarea}")


knitr::kable(obs, caption='Number of Observations', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=3)))  




```
The raw looks okay
```{r investigate_cams_status,fig.cap = "landed weight by status and year "}

status_check<-combined_dataset %>%
  group_by(status,year,market_desc) %>%
  summarise(landed_mt=sum(lndlb)/lbs_per_mt)


ggplot(status_check, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="stack", stat="identity") +
    facet_wrap(~market_desc) 

```

But the percentage based graphs show that more of the "Unclassifieds" are PZERO. 

```{r investigate_status2,fig.cap = "landed weight by status and year in percentages"}
ggplot(status_check, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="fill", stat="identity") +
    facet_wrap(~market_desc) 

```

Some states are really good for matching. Some (MA) are not.
```{r status-check2,fig.cap = "landed weight by status and year "}
status_check2<-combined_dataset %>%
  group_by(status,year,state) %>%
  summarise(landed_mt=sum(lndlb)/lbs_per_mt)

ggplot(status_check2, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="stack", stat="identity") +
    facet_wrap(~state) 
```

In percentage terms
```{r stat2_in_pct}
ggplot(status_check2, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="fill", stat="identity") +
    facet_wrap(~state) 
```

```{r status-check3}
status_check3<-combined_dataset %>%
  group_by(status,year,state, market_desc) %>%
  summarise(landed_mt=sum(lndlb)/lbs_per_mt)

ggplot(status_check3, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="stack", stat="identity") +
    facet_grid(rows=vars(state), cols=vars(market_desc)) 

```

```{r status-check3a,fig.cap = "landed weight by status and year, disaggregated by state and market category"}
status_check3<-combined_dataset %>%
  group_by(status,year,state, market_desc) %>%
  summarise(landed_mt=sum(lndlb)/lbs_per_mt)%>%
  filter(state!="ME")

ggplot(status_check3, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="stack", stat="identity") +
    facet_grid(rows=vars(state), cols=vars(market_desc)) 

```


```{r status-check3aFree,fig.cap = "landed weight by status and year, disaggregated by state and market category. Different y scaless for each state. No Maine"}
status_check3<-combined_dataset %>%
  group_by(status,year,state, market_desc) %>%
  summarise(landed_mt=sum(lndlb)/lbs_per_mt) %>%
  filter(state!="ME")
         
ggplot(status_check3, aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="stack", stat="identity") +
    facet_grid(rows=vars(state), cols=vars(market_desc), scale="free") 

```


After conditioning for state, the Unclassified seem to "match" at a similar rate to the other cats.

```{r subset3,fig.cap = "landed weight by status and year, disaggregated by state and market category.3 most common states"}
status_check3 %>%
  dplyr::filter(state %in% c('MA','NY', 'RI') ) %>%
  ggplot( aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="stack", stat="identity") +
    facet_grid(rows=vars(state), cols=vars(market_desc)) 
```


The percentage bar graphs confirm. 

```{r subset3_pct,fig.cap = "landed weight by status and year, disaggregated by state and market category.3 most common states In percentages"}
status_check3 %>%
  dplyr::filter(state %in% c('MA','NY', 'RI') ) %>%
  ggplot( aes(fill=status, y=landed_mt, x=year)) + 
    geom_bar(position="fill", stat="identity") +
    facet_grid(rows=vars(state), cols=vars(market_desc)) 
```

The proportion of PZEROs is consistent across the market categories. I'm setting aside 2024 as potentially "not complete."  It would be concerning if there were more Unclassifieds that did not match compared to the other market categories. But that doesn't seem to be happening.  However, lots of MA does not match.   The fraction of PZERO for NY and RI is less than the other cats. 

I was concerned that the unclassifieds were more likely to be PZERO if they are coming from state vessels that do not sort catch. "Real" prices remain a concern.

# Other things

Here's the result of ``skim``.

```{r summary_tables}
now<-combined_dataset %>%
    select(-c("weighting")) %>%
  filter(is.na(price)==0)

skim_results<-skim(now)
skim_results


# 
# test<-combined_dataset %>%
#   filter(is.na(StateOtherQJumbo)==1)
# 
# 
# z<-cleaned_landings %>%
#   filter(camsid=='222074_20221226120000_22207422122604')
# 
# z2<-cleaned_landings %>%
#   filter(dlr_date=='2022-12-26') %>%
#   filter(market_desc=="Large") %>%
#   arrange(stockarea)

```




# Low Prices

In my ML data prep, I've dropped observations where prices are <$0.15. This is reasonable, but they are disproportionately Unclassified. I suspect something different is going on with those observations, or we have a CAMS allocation issue intersecting with integer values. 

```{r investigate_prices,fig.cap = "Histogram of prices, weighted"}

  price_95th_percentile <- combined_dataset %>%
    summarise(price_95th = quantile(price, 0.95, na.rm = TRUE)) %>%
    pull(price_95th)
  
  
  working_dataset<-combined_dataset %>%
    filter(price<=price_95th_percentile) %>%
    filter(price>0)

working_dataset <- working_dataset %>%
  # First calculate the mean price for each market factor
  group_by(market_desc) %>%
  mutate(weighted_mean_price = weighted.mean(price, w = lndlb, na.rm = TRUE)) %>%
  ungroup() %>%
  # Reorder the factor levels based on descending mean price
  mutate(market_desc_factor_ordered = fct_reorder(market_desc, 
                                                  weighted_mean_price, 
                                                  .desc = TRUE)) %>%
  # Remove the temporary weighted_mean_price column
  select(-weighted_mean_price)


wp<-ggplot(working_dataset, aes(x = priceR_CPI)) + 
      geom_histogram(aes(weight = lndlb), boundary = 0, binwidth=0.20) + 
   labs(, x = glue("Real Price of Black Sea Bass, 2014-2020"), y = "Pounds") +
  #    theme_minimal() + 
  facet_wrap(vars(market_desc_factor_ordered), ncol=1, scales="free_y")

wp
```


# Unclassified Only Dealers 

There are a few dealers that sell almost exclusively Unclassified fish. The concern 
is that out-of-sample prediction for these firms will be invalid.

Only a small fraction of the total landings and observations are from these dealers. It's 5,000 observations in the entire time series (not shown).


For the 2018-2024 time period (i need to predict for 2019-2024, so I don't know why I included 2018.  There's about 3,000 transactions and 445,000 lbs of Unclassified landings from the Almost-always Unclassified  dealers. There 29,000,000 of total landings from the classified dealers.




```{r UncOnly1}
##########################################################################
##########################################################################
## Look at the dealers that are mostly "Unclassified"
##########################################################################
##########################################################################

unc_check <-combined_dataset %>%
  mutate(
    many_unc=case_when(
      Frac2014TUnclassified>=0.85 ~ "Mostly Unclassified",
      .default="Not"
    ))%>%
  mutate(dlrid=droplevels(dlrid))

#This isnt' exactly a transaction, because I'm calling a camsid an transaction.
unclassified_table1<-unc_check %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  group_by(many_unc) %>%
  summarise(observations=n_distinct(camsid),
            dealers=n_distinct(dlrid),
            lnd_000lbs=round(sum(lndlb)/1000,1)
            ) %>%
  mutate(lbs_per_trans=lnd_000lbs*1000/observations)

# 36 Dealers that are frequently unclassified in the 2010-2014 time period.
# There are 5100 transactions and 620000 lbs.  This is lower pounds per "transaction" than the classifiers

knitr::kable(unclassified_table1, caption='Total landings, transactions size, diaggregated into Mostly unclassified and mostly classified', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```

Excluding the dlrid=0, and there's still 27,000,000 of landings. 

```{r UncOnly2}
unclassified_table1a<-unc_check %>%
    filter(dlr_date>=mdy("01/01/2018")) %>%
  filter(dlrid!=0) %>%
    group_by(many_unc) %>%
  summarise(observations=n_distinct(camsid),
            dealers=n_distinct(dlrid),
            lnd_000lbs=round(sum(lndlb)/1000,1)) %>%
  mutate(lbs_per_trans=lnd_000lbs*1000/observations)

knitr::kable(unclassified_table1a, caption='Total landings, transactions size, diaggregated into Mostly unclassified and mostly classified. No dlrid==0 included.', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```

About 419,000 lbs of landings from the "always unclassified" dealers.

```{r Always_Unclassified}
##########################################################################
##########################################################################
## Look at the dealers that are always "Unclassified"
##########################################################################
##########################################################################

unc_check2 <-combined_dataset %>%
  mutate(
    many_unc=case_when(
      Frac2014TUnclassified==1 ~ "All Unclassified",
      .default="Not"
    ))%>%
  mutate(dlrid=droplevels(dlrid))

unclassified_table2<-unc_check2 %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  group_by(many_unc) %>%
  summarise(observations=n_distinct(camsid),
            dealers=n_distinct(dlrid),
            lnd_000lbs=round(sum(lndlb)/1000,1)) %>%
  mutate(lbs_per_trans=lnd_000lbs*1000/observations)

knitr::kable(unclassified_table2, caption='Total landings, transactions size, diaggregated into Always unclassified and mostly classified', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```

```{r Always_Unclassified_no_dlr0, eval=FALSE}

unclassified_table2a<-unc_check2 %>%
    filter(dlr_date>=mdy("01/01/2018")) %>%
  filter(dlrid!=0) %>%
  group_by(many_unc) %>%
  summarise(observations=n_distinct(camsid),
            dealers=n_distinct(dlrid),
            lnd_000lbs=round(sum(lndlb)/1000,1)) %>%
  mutate(lbs_per_trans=lnd_000lbs*1000/observations)

# Still lower when I filter out the dlrid=0 observations
knitr::kable(unclassified_table2a, caption='Total landings, transactions size, diaggregated into Always unclassified and mostly classified', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  
```

Show the dealerid's and their landed pounds 

```{r Only_UNC}
unclassified_table3<-unc_check2 %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  group_by(many_unc, dlrid, market_desc) %>%
  summarise(lnd_000lbs=round(sum(lndlb)/1000,1)) %>%
  filter(lnd_000lbs>=2) %>%
arrange(many_unc, market_desc, desc(lnd_000lbs)) 

unclassified_table4<-unclassified_table3 %>%
  ungroup() %>%
  filter(many_unc=="All Unclassified") %>%
  filter(lnd_000lbs>=6) %>%
  select(dlrid, lnd_000lbs)

knitr::kable(unclassified_table4, caption='Dealers most likely to be all unclassified', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4) ) )  

```

The amount of unclassified transactions from the "unclassifiers" relative to the total amount of unclassifieds
is also of interest. Approximately 1/4th of the unclassified landings are coming from "unclassified-only" dealers.  On average, the transactions are about 50% larger. I'm uncomfortable using prices for 2 of the largest ones: 3710 and 3290.  That's about 1/8th of the unclassified landings. Even those two might be okay.

```{r only_unc2}

################################################################################ 
# Unclassified Landings by "mostly unclassified dealers 
################################################################################ 

unc_check5 <-combined_dataset %>%
  filter(market_desc=="Unclassified") %>%
    mutate(
    many_unc=case_when(
      Frac2014TUnclassified>=0.85 ~ "Mostly Unclassified",
      .default="Not"
    ))%>%
  mutate(dlrid=droplevels(dlrid))

#This isnt' exactly a transaction, because I'm calling a camsid an transaction.
unclassified_table5<-unc_check5 %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  group_by(many_unc) %>%
  summarise(observations=n_distinct(camsid),
            dealers=n_distinct(dlrid),
            lnd_000lbs=round(sum(lndlb)/1000,1)
  ) %>%
  mutate(lbs_per_trans=lnd_000lbs*1000/observations) 


knitr::kable(unclassified_table5, caption='Unclassified landings, average transactions size, diaggregated into Mostly unclassified and not', format.args = list(big.mark = ","), digits=0, align=c("l",rep('r',times=4)))  

```

```{r prep1_for_scatter}
# Data prop for scatter plots.  
# Select the dealerids that purchased at least 6k  of unclassifieds after jan 1 2018.

unclassified_table55<-unc_check5 %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  group_by(many_unc, dlrid, market_desc) %>%
  summarise(lnd_000lbs=round(sum(lndlb)/1000,1)) %>%
  filter(lnd_000lbs>=2) %>%
  arrange(many_unc, market_desc, desc(lnd_000lbs)) 

dlrs<-unclassified_table55 %>%
  ungroup() %>%
  filter(many_unc=="Mostly Unclassified") %>%
  filter(lnd_000lbs >=6) %>%
  select(dlrid) %>%
  mutate(dlrid=droplevels(dlrid)) %>%
  pull(dlrid)


 # extract the observations for those dealers
dlr_detail<-combined_dataset %>%
  filter(dlrid %in% dlrs) %>%
  group_by(dlrid) %>%
#  mutate(price=round(price,2)) %>%
  relocate(c(market_desc,dlrid,camsid,dlr_date,price, value, lndlb, status)) %>%
  arrange(dlrid, dlr_date, mygear, hullid)
 
# Do these dealers actually report prices that vary for the unclassfieds?
# Do they have purchases from multiple vessels on the same day?
# Are they getting the same prices?
# View(dlr_detail %>% filter(dlrid %in%dlrs[2]))
```


```{r prep2_for_scatter}


#############A little more systematic ####################
# lets look at the prices of J,M,L, U of "everyone else" compared to the top 10 
# unclassified dealers
good<-combined_dataset %>%
  filter((dlrid %in% dlrs)==0) %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  group_by(dlr_date, market_desc) %>%
  summarise(lndlb=sum(lndlb),
         value=sum(value),
         type=1) %>%
  mutate(price=value/lndlb) %>%
  select(dlr_date,market_desc, price) %>%
  mutate(market_desc=as.character(market_desc)) %>%
  ungroup()

good<-good %>%
  pivot_wider(values_from=price, names_from=market_desc, names_prefix = "price_")%>%
  complete(dlr_date)

unclassified_test<-combined_dataset %>%
  filter(market_desc=="Unclassified") %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  filter((dlrid %in% dlrs)==1) %>%
  group_by(dlr_date, dlrid, market_desc) %>%
  summarise(lndlb=sum(lndlb),
            value=sum(value),
            type=2) %>%
  mutate(price=value/lndlb) %>%
  mutate(dlrid=droplevels(dlrid)) %>%
  ungroup() %>%
  complete(dlrid, dlr_date)


unclassified_test2<-unclassified_test %>%
  left_join(good, by=join_by(dlr_date==dlr_date))






unclassified_no_aggregate<-combined_dataset %>%
  filter(market_desc=="Unclassified") %>%
  filter(dlr_date>=mdy("01/01/2018")) %>%
  filter((dlrid %in% dlrs)==1) %>%
  mutate(dlrid=droplevels(dlrid)) %>%
  ungroup() %>%
  complete(dlrid, dlr_date)

unclassified_no_aggregate<-unclassified_no_aggregate %>%
  left_join(good, by=join_by(dlr_date==dlr_date))






```  

## Daily average scatterplots

The prices for two dealers (3290 and 3710) are very consistent, which does not inspire much confidence in using them to predict market category. Each of these dealers buys from 1-2 permits -- which means that they are likely integrated.   I could include a predictor, but I don't know how it should impact the classifier. I think what I actually want to do is exclude observations where a permit is exclusively selling to 1 buyer. Or downweight them.

The prices for the other dealers all fluctuate up and down, suggesting that they do 'mean' something.

Scatterplot of the average unclassified sales prices for the unclassified-only dealers against the daily average price of Jumbo. 

```{r scattter_Jumbo_baseline}
unclassified_test2  %>%
  ggplot(aes(x=dlr_date,y=price)) + 
  geom_point(aes(size=lndlb), color="blue") +
  geom_line(aes(x=dlr_date,y=price_Large), linetype=2, linewidth=.5 ) + 
  facet_wrap(vars(dlrid))
```
  
Scatterplot of the average unclassified sales prices for the unclassified-only dealers  against the daily average price of Large

```{r scattter_Large_baseline}
unclassified_test2  %>%
  ggplot(aes(x=dlr_date,y=price)) + 
  geom_point(aes(size=lndlb), color="blue") +
  geom_line(aes(x=dlr_date,y=price_Large), linetype=2, linewidth=.5 ) + 
  facet_wrap(vars(dlrid))
```

Scatterplot of the average unclassified sales prices for the unclassified-only dealers  against the daily average price of Medium

```{r scattter_Medium_baseline}

unclassified_test2  %>%
  ggplot(aes(x=dlr_date,y=price)) + 
  geom_point(aes(size=lndlb), color="blue") +
  geom_line(aes(x=dlr_date,y=price_Medium), linetype=3, linewidth=.5 ) + 
  facet_wrap(vars(dlrid))



```

## Transaction level scatterplot

Scatterplot of the transaction level  unclassified sales prices for the unclassified-only dealers against the daily average price of Jumbo

```{r no_aggregateJ}
unclassified_no_aggregate  %>%
  ggplot(aes(x=dlr_date,y=price)) + 
  geom_point(aes(size=lndlb), color="blue") +
  geom_line(aes(x=dlr_date,y=price_Jumbo), linetype=2, linewidth=.5 ) + 
  facet_wrap(vars(dlrid))
  
```
Scatterplot of the transaction level  unclassified sales prices for the unclassified-only dealers against the daily average price of Large

```{r no_aggregateL}


unclassified_no_aggregate  %>%
  ggplot(aes(x=dlr_date,y=price)) + 
  geom_point(aes(size=lndlb), color="blue") +
  geom_line(aes(x=dlr_date,y=price_Large), linetype=2, linewidth=.5 ) + 
  facet_wrap(vars(dlrid))
  
```


Scatterplot of the transaction level  unclassified sales prices for the unclassified-only dealers against the daily average price of Medium


```{r no_aggregateM}
unclassified_no_aggregate  %>%
  ggplot(aes(x=dlr_date,y=price)) + 
  geom_point(aes(size=lndlb), color="blue") +
  geom_line(aes(x=dlr_date,y=price_Medium), linetype=3, linewidth=.5 ) + 
  facet_wrap(vars(dlrid))
```

<!---
\newpage
--->


# References
<div id="refs"></div>


# Appendix{-}